\noindent Autonomous agents are becoming our next companions. They may be able to offer physical therapy assistance, play games and even help us treat weight loss. However, it is not enough to build agents that create strong first impressions. They need to continuously convey such feelings and encourage user interactions, i.e., to build and maintain rapport over long periods of time. In order to manage rapport, agents need to show signs of positivity, mutual attention and coordination during, e.g., motivate, establish eye contact, and postural mimicry, respectively. Nowadays, social agents only tackle one of these components, and the ones that do are not robotic. For this purpose, we designed an extensible rapport model that enables robotic and virtual agents to show natural signs of rapport according to the dyadic state of the interaction. The model was implemented using the \acf{SERA} ecosystem and tested using robot \ac{EMYS} on a novel scenario called Quick Numbers regarding likeability, intelligence, animacy, and proximity. There is no statistical significance on the obtained results, however, from the video footage, we noticed that the participants manifested more positive reactions and emotions in the rapport condition, therefore, a sample higher than 20 might reveal the expected statistical results. Finally, researchers may integrate the rapport model on any \ac{SERA} agent with low effort.




