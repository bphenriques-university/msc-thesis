@article{Dayan1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
author = {Dayan, Peter},
doi = {10.1007/978-1-4615-3618-5{\_}4},
file = {:Users/brunohenriques/Documents/ThesisPapers/Dayan/Unknown/Dayan - 1992 - Technical Note Q , -Learning.pdf:pdf},
isbn = {978-1-4613-6608-9},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {asynchronous dynamic programming,q learning,reinforcement learning,temporal differences},
number = {3},
pages = {279--292},
title = {{Technical Note Q-Learning}},
url = {http://www.springerlink.com/index/t774414027552160.pdf},
volume = {292},
year = {1992}
}
@book{Bishop2006,
author = {Bishop, Christopher M},
isbn = {0387310738},
publisher = {springer},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
@article{Schroder2012,
author = {Schr{\"{o}}der, Marc and Bevacqua, Elisabetta and Cowie, Roddy and Eyben, Florian and Gunes, Hatice and Heylen, Dirk and Maat, Mark Ter and McKeown, Gary and Pammi, Sathish and Pantic, Maja},
issn = {1949-3045},
journal = {Affective Computing, IEEE Transactions on},
number = {2},
pages = {165--183},
publisher = {IEEE},
title = {{Building Autonomous Sensitive Artificial Listeners}},
volume = {3},
year = {2012}
}
@incollection{Atkeson1997,
author = {Atkeson, Christopher G and Moore, Andrew W and Schaal, Stefan},
booktitle = {Lazy learning},
isbn = {904814860X},
pages = {75--113},
publisher = {Springer},
title = {{Locally Weighted Learning For Control}},
year = {1997}
}
@article{Ribeiro2014,
abstract = {We are addressing the problem of creating empathic robot tutors to support school students studying geography topics on a multi- touch table. A multi-role serious game Enercities-2 has been developed from an earlier single-user version in which a Mayor, Economist and Environmentalist have control over differing resources. The game explores the tension between individual and collaborative success. A robot tutor, embodied as a NAO Torso robot, can play any one of these roles. This interactive demo using a large tablet and the NAO will allow attendees to play with the robot, which currently tries to maximize the collaborative score.},
author = {Ribeiro, Tiago and Pereira, Andr{\'{e}} and Deshmukh, Amol and Aylett, Ruth and Paiva, Ana},
file = {:Users/brunohenriques/Documents/ThesisPapers/Ribeiro et al/International conference on Autonomous agents and multi-agent systems/Ribeiro et al. - 2014 - I’m the Mayor A Robot Tutor in Enercities-2.pdf:pdf},
isbn = {978-1-4503-2738-1},
journal = {International conference on Autonomous agents and multi-agent systems},
keywords = {1,domain and scenario,human-robot interaction,serious games,social robots},
pages = {1675--1676},
title = {{I’m the Mayor: A Robot Tutor in Enercities-2}},
volume = {2},
year = {2014}
}
@article{Steinfeld2009,
abstract = {The Wizard of Oz experiment method has a long tradition of acceptance and use within the field of human-robot interaction. The community has traditionally downplayed the importance of interaction evaluations run with the inverse model: the human simulated to evaluate robot behavior, or Oz of Wizard. We argue that such studies play an important role in the field of human-robot interaction. We differentiate between methodologically rigorous human modeling and placeholder simulations using simplified human models. Guidelines are proposed for when Oz of Wizard results should be considered acceptable. This paper also describes a framework for describing the various permutations of Wizard and Oz states.},
author = {Steinfeld, Aaron and Jenkins, Odest Chadwicke and Scassellati, Brian},
doi = {10.1145/1514095.1514115},
file = {:Users/brunohenriques/Documents/ThesisPapers/Steinfeld, Jenkins, Scassellati/Proceedings of the 4th ACMIEEE international conference on Human robot interaction/Steinfeld, Jenkins, Scassellati - 2009 - The Oz of Wizard Simulating the Human for Interaction Research.pdf:pdf},
isbn = {9781605584041},
journal = {Proceedings of the 4th ACMIEEE international conference on Human robot interaction},
keywords = {Human-robot interaction,Wizard of Oz,evaluation,example,experimental,hci,hri,human robot interaction,human-robot interaction,interaction,methodology,oz technique,research using wizard,several papers recent,widespread within,wizard of oz,wizard oz},
pages = {101--107},
title = {{The Oz of Wizard: Simulating the Human for Interaction Research}},
url = {http://portal.acm.org/citation.cfm?id=1514115},
year = {2009}
}
@incollection{Nielsen1992,
abstract = {An abstract is not available.},
author = {Nielsen, Jakob},
booktitle = {Advances in human-computer interaction},
isbn = {0-89391-751-6},
keywords = {usability{\_}evaluation{\_}methods},
pages = {69--82},
publisher = {Ablex Publishing Corp.},
title = {{Evaluating the Thinking-Aloud Technique for use by Computer Scientists}},
url = {citeulike-article-id:1426806$\backslash$nhttp://portal.acm.org/citation.cfm?id=159440},
year = {1992}
}
@article{Nardi1996,
author = {Nardi, BA},
file = {:Users/brunohenriques/Documents/ThesisPapers/Nardi/{\ldots} consciousness Activity theory and human-computer {\ldots}/Nardi - 1996 - Activity Theory and Human-Computer Interaction.pdf:pdf},
isbn = {0-262-14058-6},
journal = {{\ldots} consciousness: Activity theory and human-computer {\ldots}},
pages = {4--8},
title = {{Activity Theory and Human-Computer Interaction}},
year = {1996}
}
@article{Bailenson2001,
abstract = {During the last half of the twentieth century, psychologists and anthropologists have studied proxemics, or spacing behavior, among people in many contexts. As we enter the twenty-first century, immersive virtual environment technology promises new experimental venues in which researchers can study proxemics. Immersive virtual environments provide realistic and compelling experimental settings without sacrificing experimental control. The experiment reported here tested Argyle and Dean’s (1965) equilibrium theory specification of an inverse relationship between mutual gaze, a non-verbal cue signaling intimacy, and interpersonal distance. Participants were immersed in a three-dimensional virtual room in which a virtual human representation (i.e., an embodied agent) stood. Under the guise of a memory task, participants walked towards and around the agent. Distance between the participant and agent was tracked automatically via our immersive virtual environment system. All participants maintained more space around agents than around similarly sized and shaped but non-human like objects. Female participants maintained more interpersonal distance between themselves and agents who engaged them in eye contact (i.e., mutual gaze behavior) than agents who did not engage them in eye contact while male participants did not. Implications are discussed for the study of proxemics via immersive virtual environment technology as well as the design of virtual environments and virtual humans},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bailenson, Jeremy N. and Blascovich, Jim and Beall, Andrew C. and Loomis, Jack M.},
doi = {10.1162/105474601753272844},
eprint = {arXiv:1011.1669v3},
file = {:Users/brunohenriques/Documents/ThesisPapers/Bailenson et al/Presence Teleoperators and Virtual Environments/Bailenson et al. - 2001 - Equilibrium Theory Revisited Mutual Gaze and Personal Space in Virtual Environments.pdf:pdf},
isbn = {1054-7460},
issn = {1054-7460},
journal = {Presence: Teleoperators and Virtual Environments},
number = {6},
pages = {583--598},
pmid = {25246403},
title = {{Equilibrium Theory Revisited: Mutual Gaze and Personal Space in Virtual Environments}},
volume = {10},
year = {2001}
}
@article{Poppe2010,
author = {Poppe, Ronald and Truong, Khiet P and Reidsma, Dennis and Heylen, Dirk},
file = {:Users/brunohenriques/Documents/ThesisPapers/Poppe et al/English/Poppe et al. - 2010 - Backchannel Strategies for Artificial Listeners.pdf:pdf},
journal = {English},
number = {211486},
pages = {146--158},
title = {{Backchannel Strategies for Artificial Listeners}},
volume = {211486},
year = {2010}
}
@phdthesis{Watkins1989,
author = {Watkins, C},
publisher = {Ph. D. thesis, Cambridge University},
title = {{Models of Delayed Reinforcement Learning}},
year = {1989}
}
@article{Ng2000,
abstract = {This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal behaviour. IRL may be useful for apprenticeship learning to acquire skilled behaviour, and for ascertaining the reward function being optimized by a natural system. We rst characterize the set of all reward functions for which a given policy is optimal. We then derive three algorithms for IRL. The rst two deal with the case where the entire policy is known; we handle tabulated reward functions on a nite state space and linear functional approximation of the reward function over a potentially in- nite state space. The third algorithm deals with the more realistic case in which the policy is known only through a nite set of observed trajectories. In all cases, a key issue is degeneracythe existence of a large set of reward functions for which the observed policy is optimal. To remove...},
author = {Ng, Andrew and Russell, Stuart},
doi = {10.2460/ajvr.67.2.323},
file = {:Users/brunohenriques/Documents/ThesisPapers/Ng, Russell/Proceedings of the Seventeenth International Conference on Machine Learning/Ng, Russell - 2000 - Algorithms for Inverse Reinforcement Learning.pdf:pdf},
isbn = {1-55860-707-2},
issn = {00029645},
journal = {Proceedings of the Seventeenth International Conference on Machine Learning},
pages = {663--670},
pmid = {16454640},
title = {{Algorithms for Inverse Reinforcement Learning}},
url = {http://www-cs.stanford.edu/people/ang/papers/icml00-irl.pdf},
volume = {0},
year = {2000}
}
@article{Peters2005,
abstract = {One of the major problems of user's interaction with Embodied Conversational Agents (ECAs) is to have the conversation last more than few second: after being amused and intrigued by the ECAs, users may find rapidly the restrictions and limitations of the dialog systems, they may perceive the repetition of the ECAs animation, they may find the behaviors of ECAs to be inconsistent and implausible, etc. We believe that some special links, or bonds, have to be established between users and ECAs during interaction. It is our view that showing and/or perceiving interest is the necessary premise to establish a relationship. In this paper we present a model of an ECA able to establish, maintain and end the conversation based on its perception of the level of interest of its interlocutor.},
author = {Peters, Christopher and Pelachaud, Catherine and Bevacqua, Elisabetta and Mancini, Maurizio and Poggi, Isabella},
doi = {10.1007/11550617{\_}20},
file = {:Users/brunohenriques/Documents/ThesisPapers/Peters et al/Lecture Notes in Computer Science/Peters et al. - 2005 - A Model of Attention and Interest Using Gaze Behavior.pdf:pdf},
isbn = {9783540287384},
issn = {0954-3945},
journal = {Lecture Notes in Computer Science},
pages = {229--240},
title = {{A Model of Attention and Interest Using Gaze Behavior}},
year = {2005}
}
@article{Blumberg2002,
abstract = {The ability to learn is a potentially compelling and important quality for interactive synthetic characters. To that end, we describe a practical approach to real-time learning for synthetic characters. Our implementation is grounded in the techniques of reinforcement learning and informed by insights from animal training. It simplifies the learning task for characters by (a) enabling them to take advantage of predictable regularities in their world, (b) allowing them to make maximal use of any supervisory signals, and (c) making them easy to train by humans.We built an autonomous animated dog that can be trained with a technique used to train real dogs called "clicker training". Capabilities demonstrated include being trained to recognize and use acoustic patterns as cues for actions, as well as to synthesize new actions from novel paths through its motion space.A key contribution of this paper is to demonstrate that by addressing the three problems of state, action, and state-action space discovery at the same time, the solution for each becomes easier. Finally, we articulate heuristics and design principles that make learning practical for synthetic characters.},
author = {Blumberg, Bruce and Downie, Marc and Ivanov, Yuri and Berlin, Matt and Johnson, Michael Patrick and Tomlinson, Bill},
doi = {10.1145/566570.566597},
file = {:Users/brunohenriques/Documents/ThesisPapers/Blumberg et al/Proceedings of the 29th annual conference on Computer graphics and interactive techniques/Blumberg et al. - 2002 - Integrated Learning for Interactive Synthetic Characters.pdf:pdf},
isbn = {1581135211},
issn = {07300301},
journal = {Proceedings of the 29th annual conference on Computer graphics and interactive techniques},
keywords = {animation,behavioral animation,computer games},
pages = {417},
pmid = {16475432339575221692},
title = {{Integrated Learning for Interactive Synthetic Characters}},
url = {http://portal.acm.org/citation.cfm?doid=566570.566597},
year = {2002}
}
@inproceedings{Kok2011,
author = {Kok, Iwan De and Heylen, Dirk and de Kok, Iwan},
booktitle = {Affective Computing and Intelligent Interaction},
isbn = {978-3-642-24599-2},
pages = {477--486},
publisher = {Springer},
title = {{When Do We Smile? Analysis and Modeling of the Nonverbal Context of Listener Smiles in Conversation}},
volume = {6974},
year = {2011}
}
@article{Bartneck2009,
abstract = {This study emphasizes the need for standardized measurement tools for human robot interaction (HRI). If we are to make progress in this field then we must be able to compare the results from different studies. A literature review has been performed on the measurements of five key concepts in HRI: anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety. The results have been distilled into five consistent questionnaires using semantic differential scales. We report reliability and validity indicators based on several empirical studies that used these questionnaires. It is our hope that these questionnaires can be used by robot developers to monitor their progress. Psychologists are invited to further develop the questionnaires by adding new concepts, and to conduct further validations where it appears necessary.},
author = {Bartneck, Christoph and Kuli{\'{c}}, Dana and Croft, Elizabeth and Zoghbi, Susana},
doi = {10.1007/s12369-008-0001-3},
file = {:Users/brunohenriques/Documents/ThesisPapers/Bartneck et al/International Journal of Social Robotics/Bartneck et al. - 2009 - Measurement Instruments for the Anthropomorphism, Animacy, Likeability, Perceived Intelligence, and Perceived S.pdf:pdf},
isbn = {1875479118754805},
issn = {18754791},
journal = {International Journal of Social Robotics},
keywords = {Human factors,Measurement,Perception,Robot},
number = {1},
pages = {71--81},
title = {{Measurement Instruments for the Anthropomorphism, Animacy, Likeability, Perceived Intelligence, and Perceived Safety of Robots}},
url = {http://link.springer.com/10.1007/s12369-008-0001-3},
volume = {1},
year = {2009}
}
@misc{Davis1980,
abstract = {The Interpersonal Reactivity Index is a measure of dispositional empathy that takes as its starting point the notion that empathy consists of a set of separate but related constructs. The instrument contains four seven- item subscales, each tapping a separate facet of empathy.},
author = {Davis, Mark H},
booktitle = {JSAS Catalog of Selected Documents in Psychology},
doi = {10.1037/t01093-000},
issn = {0488-1281},
keywords = {Empathy,Interpersonal Reactivity},
pages = {14--15},
title = {{Interpersonal Reactivity Index}},
volume = {10},
year = {1980}
}
@article{Hall2009,
abstract = {Abstract More than twelve years have elapsed since the first public release of WEKA . In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread ... $\backslash$n},
author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H},
doi = {10.1145/1656274.1656278},
file = {:Users/brunohenriques/Documents/ThesisPapers/Hall et al/ACM SIGKDD Explorations/Hall et al. - 2009 - The WEKA Data Mining Software An Update.pdf:pdf},
isbn = {1931-0145},
issn = {19310145},
journal = {ACM SIGKDD Explorations},
number = {1},
pages = {10--18},
title = {{The WEKA Data Mining Software: An Update}},
url = {http://portal.acm.org/citation.cfm?doid=1656274.1656278$\backslash$npapers2://publication/doi/10.1145/1656274.1656278},
volume = {11},
year = {2009}
}
@inproceedings{Lee2006,
author = {Lee, Jina and Marsella, Stacy},
booktitle = {Intelligent virtual agents},
file = {:Users/brunohenriques/Documents/ThesisPapers/Lee, Marsella/Intelligent virtual agents/Lee, Marsella - 2006 - Nonverbal Behavior Generator for Embodied Conversational Agents.pdf:pdf},
pages = {243--255},
title = {{Nonverbal Behavior Generator for Embodied Conversational Agents}},
year = {2006}
}
@article{Leuski2011,
author = {Leuski, Anton and Traum, David},
file = {:Users/brunohenriques/Documents/ThesisPapers/Leuski, Traum/AI Magazine/Leuski, Traum - 2011 - Creating Virtual Human Dialogue Using Information Retrieval Techniques.pdf:pdf},
journal = {AI Magazine},
pages = {42--56},
title = {{Creating Virtual Human Dialogue Using Information Retrieval Techniques}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/view/2347},
year = {2011}
}
@misc{Sacks1974,
abstract = {The organization of taking tums to talk is fondamental to conversation,$\backslash$nas well as to other speech-exchange Systems. A model for th{\'{e}} turn-taking$\backslash$norganization for conversation is proposed, and is examined for its$\backslash$ncompatibility with a list of grossly observable facts about conversation.$\backslash$nThe r{\'{e}}sulta of th{\'{e}} examinat{\^{\i}}on suggest that, at least, a model for$\backslash$nturn-taking in conversation will be characterized as locally mau-aged,$\backslash$nparty-administered, interactionally controlled, and sensitive to$\backslash$nr{\'{e}}cipient design. Several g{\^{e}}nerai cons{\'{e}}quences of th{\'{e}} model are explicated,$\backslash$nand contrasts are sketched with turn-taking organizations for other$\backslash$nspeech-exchange Systems},
author = {Sacks, Harvey and Schegloff, Emanuel a and Jefferson, Gail},
booktitle = {Language},
doi = {10.2307/412243},
file = {:Users/brunohenriques/Documents/ThesisPapers/Sacks, Schegloff, Jefferson/Language/Sacks, Schegloff, Jefferson - 1974 - A Simplest Systematics for the Organization of Turn Taking for Conversation.pdf:pdf},
isbn = {1111111111},
issn = {00978507},
keywords = {analyse de conversation,turn-taking},
pages = {696--735},
pmid = {14598623},
title = {{A Simplest Systematics for the Organization of Turn Taking for Conversation}},
volume = {50},
year = {1974}
}
@article{Howes2011,
abstract = {Spoken contributions in dialogue often continue or complete earlier contributions by either the same or a different speaker. These compound contributions (CCs) thus provide a natural context for investigations of incremental processing in dialogue. We present a corpus study which confirms that CCs are a key dialogue phenomenon: almost 20{\%} of contributions fit our general definition of CCs, with nearly 3{\%} being the cross-person case most often studied. The results suggest that processing is word-by-word incremental, as splits can occur within syntactic ‘constituents’; however, some systematic differences between sameand cross-person cases indicate important dialogue-specific pragmatic effects. An experimental study then investigates these effects by artificially introducing CCs into multi-party text dialogue. Results suggest that CCs affect people’s expectations about who will speak next and whether other participants have formed a coalition or ‘party’. Together, these studies suggest that CCs require an incremental processing mechanism that can provide a resource for constructing linguistic constituents that span multiple contributions and multiple participants. They also suggest the need to model higher-level dialogue units that have consequences for the organisation of turn-taking and for the development of a shared context.},
author = {Howes, Christine and Purver, Matthew and Healey, Patrick G T and Mills, Gregory J},
doi = {10.5087/dad.2011.111},
file = {:Users/brunohenriques/Documents/ThesisPapers/Howes et al/Dialogue and Discourse/Howes et al. - 2011 - On Incrementality in Dialogue Evidence from Compound Contributions ∗.pdf:pdf},
issn = {2152-9620},
journal = {Dialogue and Discourse},
number = {1},
pages = {279--310},
title = {{On Incrementality in Dialogue: Evidence from Compound Contributions ∗}},
volume = {2},
year = {2011}
}
@article{Shapiro2011,
abstract = {We describe a system for animating virtual characters that encompasses many important aspects of character modeling for simu- lations and games. These include locomotion, facial animation, speech synthesis, reaching/grabbing, and various automated non-verbal behav- iors, such as nodding, gesturing and eye saccades. Our systemimplements aspects of character animation from the research community that yield high levels of realism and control.},
author = {Shapiro, Ari},
doi = {10.1007/978-3-642-25090-3{\_}9},
file = {:Users/brunohenriques/Documents/ThesisPapers/Shapiro/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Shapiro - 2011 - Building a Character Animation System.pdf:pdf},
isbn = {9783642250897},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {animation,character,graphics,system},
pages = {98--109},
pmid = {4676330},
title = {{Building a Character Animation System}},
volume = {7060 LNCS},
year = {2011}
}
@incollection{Gratch2006,
author = {Gratch, Jonathan and Okhmatovskaia, Anna and Lamothe, Francois and Marsella, Stacy and Morales, Mathieu and van der Werf, R. J. and Morency, Louis-Philippe},
booktitle = {Intelligent Virtual Agents},
doi = {10.1007/11821830},
file = {:Users/brunohenriques/Documents/ThesisPapers/Gratch et al/Intelligent Virtual Agents/Gratch et al. - 2006 - Virtual Rapport.pdf:pdf},
isbn = {978-3-540-37593-7},
pages = {14--27},
title = {{Virtual Rapport}},
volume = {4133},
year = {2006}
}
@article{Schroder2010a,
abstract = {This paper presents the SEMAINE API, an open source framework for building emotion-oriented systems. By encouraging and simplifying the use of standard representation formats, the framework aims to contribute to interoperability and reuse of system components in the research community. By providing a Java and C++ wrapper around a message-oriented middleware, the API makes it easy to integrate components running on di erent operating systems and written in di erent programming languages. The SEMAINE system 1.0 is presented as an example of a full-scale system built on top of the SEMAINE API. Three small example systems are described in detail to illustrate how integration between existing and new components is realised with minimal e ort.},
author = {Schr{\"{o}}der, Marc},
doi = {10.1155/2010/319406},
file = {:Users/brunohenriques/Documents/ThesisPapers/Schr{\"{o}}der/Advances in HumanComputer Interaction/Schr{\"{o}}der - 2010 - The SEMAINE API Towards a Standards-Based Framework for Building Emotion-Oriented Systems(2).pdf:pdf},
isbn = {1687-5893},
issn = {16875893},
journal = {Advances in HumanComputer Interaction},
pages = {1--22},
title = {{The SEMAINE API: Towards a Standards-Based Framework for Building Emotion-Oriented Systems}},
url = {http://www.hindawi.com/journals/ahci/2010/319406.html},
volume = {2010},
year = {2010}
}
@article{Bargh2002,
abstract = {Those who feel better able to express their “true selves” in Internet rather than face-to-face interaction settings are more likely to form close relationships with people met on the Internet (McKenna, Green, {\&} Gleason, this issue). Building on these correlational findings from survey data, we conducted three laboratory experiments to directly test the hypothesized causal role of differential self-expression in Internet relationship formation. Experiments 1 and 2, using a reaction time task, found that for university undergraduates, the true-self concept is more accessible in memory during Internet interactions, and the actual self more accessible during face-to-face interactions. Experiment 3 confirmed that people randomly assigned to interact over the Internet (vs. face to face) were better able to express their true-self qualities to their partners.},
author = {Bargh, John A. and McKenna, Katelyn Y.A. and Fitzsimons, Grainne M.},
doi = {10.1111/1540-4560.00247},
file = {:Users/brunohenriques/Documents/ThesisPapers/Bargh, McKenna, Fitzsimons/Journal of social {\ldots}/Bargh, McKenna, Fitzsimons - 2002 - Can You See the Real Me Activation and Expression of the “True Self” on the Internet.pdf:pdf},
issn = {0022-4537},
journal = {Journal of social {\ldots}},
number = {1},
pages = {33--48},
title = {{Can You See the Real Me? Activation and Expression of the “True Self” on the Internet}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1540-4560.00247/full},
volume = {58},
year = {2002}
}
@article{Riek2012,
abstract = {Many researchers useWizard of Oz (WoZ) as an experimental technique, but there are methodological concerns over its use, and no comprehensive criteria on how to best employ it. We systematically review 54WoZ experiments published in the primary HRI publication venues from 2001 - 2011. Using criteria proposed by Fraser and Gilbert (1991), Green et al. (2004), Steinfeld et al. (2009), and Kelley (1984), we analyzed how researchers conducted HRI WoZ experiments. Researchers mainly used WoZ for verbal (72.2{\%}) and non-verbal (48.1{\%}) processing. Most constrained wizard production (90.7{\%}), but few constrained wizard recognition (11{\%}). Few reported measuring wizard error (3.7{\%}), and few reported pre-experiment wizard training (5.4{\%}). Few reported using WoZ in an iterative manner (24.1{\%}). Based on these results we propose new reporting guidelines to aid future research.},
annote = {From Duplicate 1 (Wizard of Oz Studies in HRI: A Systematic Review and New Reporting Guidelines - Riek, Laurel)

The usage 
48.1{\%} (n= 26) papers usaram Wizard of Oz to genete some sort of vicible utterance (i.e. non-verbal behavior)

Tem queser possibvel simular the novo o sistema no futuro dadas as limitacoes do ser humano.

Ser especific{\'{a}}vel

Constraineed Wizard recognition ability ?
Constrained Wizard production ability and Wizard errors.

N{\~{a}}o h{\'{a}} muitos papers (apenas 2) falam em fazer erros.},
author = {Riek, Laurel},
doi = {10.5898/JHRI.1.1.Riek},
file = {:Users/brunohenriques/Documents/ThesisPapers/Riek/Journal of Human-Robot Interaction/Riek - 2012 - Wizard of Oz Studies in HRI A Systematic Review and New Reporting Guidelines.pdf:pdf},
issn = {21630364},
journal = {Journal of Human-Robot Interaction},
keywords = {human-robot interaction,method-,reporting guidelines,systematic review,wizard of oz},
number = {1},
pages = {119--136},
title = {{Wizard of Oz Studies in HRI: A Systematic Review and New Reporting Guidelines}},
volume = {1},
year = {2012}
}
@inproceedings{Kopp2007,
abstract = {Just like humans, conversational computer systems should not listen silently to their input and then respond. Instead, they should enforce the speaker-listener link by attending actively and giving feedback on an utterance while perceiving it. Most existing systems produce direct feedback responses to decisive (e.g. prosodic) cues. We present a framework that conceives of feedback as a more complex system, resulting from the interplay of conventionalized responses to eliciting speaker events and the multimodal behavior that signals how internal states of the listener evolve. A model for producing such incremental feedback, based on multi-layered processes for perceiving, understanding, and evaluating input, is described.},
author = {Kopp, Stefan and Stocksmeier, Thorsten and Gibbon, Dafydd},
booktitle = {Intelligent Virtual Agents},
doi = {http://dx.doi.org/10.1007/978-3-540-74997-4{\_}13},
file = {:Users/brunohenriques/Documents/ThesisPapers/Kopp, Stocksmeier, Gibbon/Intelligent Virtual Agents/Kopp, Stocksmeier, Gibbon - 2007 - Incremental Multimodal Feedback for Conversational Agents.pdf:pdf},
isbn = {9783540749967},
pages = {139--146},
title = {{Incremental Multimodal Feedback for Conversational Agents}},
year = {2007}
}
@article{Yu2013,
abstract = {In this paper we focus on modeling friendships between humans as a way of working towards technology that can initiate and sustain a lifelong relationship with users. We do this by predicting friendship status in a dyad using a set of automatically harvested verbal and nonverbal features from videos of the interaction of students in a peer tutoring study. We propose a new computational model used to model friendship status in our data, based on a group sparse model (GSM) with L2,1 norm which is designed to accommodate the sparse and noisy properties of the multi-channel features. Our GSM model achieved the best overall performance compared to a non-sparse linear model (NLM) and a regular sparse linear model (SLM), as well as outperforming human raters. Dyadic features, such as number and length of conversational turns and mutual gaze, in addition to low level features such as F0 and gaze at task, were found to be good predictors of friendship status.},
author = {Yu, Zhou and Gerritsen, David and Ogan, Amy and Black, Aw and Cassell, Justine},
file = {:Users/brunohenriques/Documents/ThesisPapers/Yu et al/Proceedings of the SIGDIAL 2013 Conference/Yu et al. - 2013 - Automatic Prediction of Friendship via Multi-model Dyadic Features.pdf:pdf},
journal = {Proceedings of the SIGDIAL 2013 Conference},
number = {August},
pages = {51--60},
title = {{Automatic Prediction of Friendship via Multi-model Dyadic Features}},
url = {http://www.sigdial.org/workshops/conference14/proceedings/pdf/SIGDIAL07.pdf},
year = {2013}
}
@article{Cakmak2012,
abstract = {Programming new skills on a robot should takeminimal time and effort. One approach to achieve this goal is to allow the robot to ask questions. This idea, called Active Learning, has recently caught a lot of attention in the robotics commu- nity. However, it has not been explored from a human-robot interaction perspective. In this paper, we identify three types of questions (label, demonstration and feature queries) and discuss how a robot can use these while learning new skills. Then, we present an experiment on human question asking which characterizes the extent to which humans use these question types. Finally, we evaluate the three question types within a human-robot teaching interaction. We inves- tigate the ease with which different types of questions are answered and whether or not there is a general preference of one type of question over another. Based on our findings from both experiments we provide guidelines for designing question asking behaviors on a robot learner.},
author = {Cakmak, Maya and Thomaz, Andrea L},
doi = {10.1145/2157689.2157693},
file = {:Users/brunohenriques/Documents/ThesisPapers/Cakmak, Thomaz/Proceedings of the seventh annual ACMIEEE international conference on Human-Robot Interaction - HRI '12/Cakmak, Thomaz - 2012 - Designing Robot Learners that Ask Good Questions.pdf:pdf},
isbn = {9781450310635},
issn = {2167-2121},
journal = {Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction - HRI '12},
keywords = {active learning,learning from demonstration},
pages = {17},
title = {{Designing Robot Learners that Ask Good Questions}},
year = {2012}
}
@article{Kang2011,
abstract = {In this paper, we describe our findings from research designed to explore the effect of self-disclosure between virtual human counselors (interviewers) and human users (interviewees) on users' social responses in counseling sessions. To investigate this subject, we designed an experiment involving three conditions of self-disclosure: high-disclosure, low-disclosure, and non-disclosure. We measured users' sense of co-presence and social attraction to virtual counselors. The results demonstrated that users reported more co-presence and social attraction to virtual humans who disclosed highly intimate information about themselves than when compared to other virtual humans who disclosed less intimate or no information about themselves. In addition, a further analysis of users' verbal self-disclosure showed that users revealed a medium level of personal information more often when interacting with virtual humans that highly-disclosed about themselves, than when interacting with virtual humans disclosing less intimate or no information about themselves.},
author = {Kang, Sin-Hwa and Gratch, Jonathan},
file = {:Users/brunohenriques/Documents/ThesisPapers/Kang, Gratch/Studies in health technology and informatics/Kang, Gratch - 2011 - People Like Virtual Counselors that Highly-Disclose About Themselves.pdf:pdf},
issn = {0926-9630},
journal = {Studies in health technology and informatics},
keywords = {Adult,Computer Simulation,Counseling,Female,Humans,Interpersonal Relations,Male,Self Disclosure,User-Computer Interface},
pages = {143--8},
pmid = {21685657},
title = {{People Like Virtual Counselors that Highly-Disclose About Themselves}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21685657},
volume = {167},
year = {2011}
}
@article{Quigley2009,
abstract = {This paper gives an overview of ROS, an open- source robot operating system. ROS is not an operating system in the traditional sense of process management and scheduling; rather, it provides a structured communications layer above the host operating systems of a heterogenous compute cluster. In this paper, we discuss how ROS relates to existing robot software frameworks, and briefly overview some of the available application software which uses ROS},
author = {Quigley, Morgan and Conley, Ken and Gerkey, Brian and FAust, Josh and Foote, Tully and Leibs, Jeremy and Berger, Eric and Wheeler, Rob and Mg, Andrew},
doi = {http://www.willowgarage.com/papers/ros-open-source-robot-operating-system},
file = {:Users/brunohenriques/Documents/ThesisPapers/Quigley et al/Icra/Quigley et al. - 2009 - ROS an open-source Robot Operating System.pdf:pdf},
isbn = {0165-022X (Print)$\backslash$r0165-022X (Linking)},
issn = {0165022X},
journal = {Icra},
number = {Figure 1},
pages = {5},
pmid = {8844323},
title = {{ROS: an open-source Robot Operating System}},
url = {http://pub1.willowgarage.com/{~}konolige/cs225B/docs/quigley-icra2009-ros.pdf},
volume = {3},
year = {2009}
}
@article{Kopp2006,
abstract = {This paper describes an international effort to unify a multimodal behavior generation framework for Embodied Conversational Agents (ECAs). We propose a three stage model we call SAIBA where the stages represent intent planning, behavior planning and behavior realization. A Function Markup Language (FML), describing intent without referring to physical behavior, mediates between the first two stages and a Behavior Markup Language (BML) describing desired physical realization, mediates between the last two stages. In this paper we will focus on BML. The hope is that this abstraction and modularization will help ECA researchers pool their resources to build more sophisticated virtual humans.},
author = {Kopp, Stefan and Krenn, Brigitte and Marsella, Stacy and Marshall, A and Pelachaud, C and Pirker, H and Th{\'{o}}risson, K and Vilhj{\'{a}}lmsson, H},
doi = {10.1007/11821830},
file = {:Users/brunohenriques/Documents/ThesisPapers/Kopp et al/Intelligent Virtual Agents/Kopp et al. - 2006 - Towards a Common Framework for Multimodal Generation The Behavior Markup Language.pdf:pdf},
isbn = {9783540375937},
issn = {03029743},
journal = {Intelligent Virtual Agents},
pages = {205--217},
title = {{Towards a Common Framework for Multimodal Generation: The Behavior Markup Language}},
url = {http://www.springerlink.com/index/T51446046010R777.pdf},
volume = {4133},
year = {2006}
}
@article{Zwiers2011,
author = {Zwiers, Job and Welbergen, Herwin Van},
file = {:Users/brunohenriques/Documents/ThesisPapers/Zwiers, Welbergen/Intelligent Virtual Agents/Zwiers, Welbergen - 2011 - Continuous Interaction within the SAIBA Framework.pdf:pdf},
isbn = {978-3-642-23973-1},
journal = {Intelligent Virtual Agents},
pages = {324--330},
title = {{Continuous Interaction within the SAIBA Framework}},
url = {http://www.springerlink.com/index/L127348H040J5383.pdf},
year = {2011}
}
@article{Hartholt2013,
abstract = {While virtual humans are proven tools for training, education and research, they are far from realizing their full potential. Advances are needed in individual capabilities, such as character animation and speech synthesis, but perhaps more importantly, fundamental questions remain as to how best to integrate these capabilities into a single framework that allows us to efficiently create characters that can engage users in meaningful and realistic social interactions. This integration requires in-depth, inter-disciplinary understanding few individuals, or even teams of individuals, possess. We help address this challenge by introducing the ICT Virtual Human Toolkit, which offers a flexible framework for exploring a variety of different types of virtual human systems, from virtual listeners and question-answering characters to virtual role-players. We show that due to its modularity, the Toolkit allows researchers to mix and match provided capabilities with their own, lowering the barrier of entry to this multi-disciplinary research challenge. © 2013 Springer-Verlag.},
author = {Hartholt, Arno and Traum, David and Marsella, Stacy C. and Shapiro, Ari and Stratou, Giota and Leuski, Anton and Morency, Louis Philippe and Gratch, Jonathan},
doi = {10.1007/978-3-642-40415-3{\_}33},
file = {:Users/brunohenriques/Documents/ThesisPapers/Hartholt et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Hartholt et al. - 2013 - All together Now Introducing the Virtual Human Toolkit.pdf:pdf},
isbn = {9783642404146},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Architectures,Audio-Visual Sensing,Embodied Conversational Agents,Natural Language Processing,Nonverbal Behavior,Real-Time Integrated Systems,Standards,Toolkits,Virtual Humans},
pages = {368--381},
title = {{All together Now: Introducing the Virtual Human Toolkit}},
volume = {8108 LNAI},
year = {2013}
}
@article{Thomaz2006,
author = {Thomaz, A.L. and Breazeal, Cynthia},
file = {:Users/brunohenriques/Documents/ThesisPapers/Thomaz, Breazeal/AAAI/Thomaz, Breazeal - 2006 - Reinforcement Learning with Human Reachers Evidence of Feedback and Guidance with Implications for Learning Pe.pdf:pdf},
isbn = {1577352815},
journal = {AAAI},
keywords = {computer vision,robotics},
pages = {1000--1005},
title = {{Reinforcement Learning with Human Reachers: Evidence of Feedback and Guidance with Implications for Learning Performance}},
url = {http://www.aaai.org/Papers/AAAI/2006/AAAI06-157.pdf},
volume = {6},
year = {2006}
}
@article{Cakmak2010,
abstract = {This paper addresses some of the problems that arise when applying active learning to the context of human-robot interaction (HRI). Active learning is an attractive strategy for robot learners because it has the potential to improve the accuracy and the speed of learning, but it can cause issues from an interaction perspective. Here we present three interaction modes that enable a robot to use active learning queries. The three modes differ in when they make queries: the first makes a query every turn, the second makes a query only under certain conditions, and the third makes a query only when explicitly requested by the teacher. We conduct an experiment in which 24 human subjects teach concepts to our upper-torso humanoid robot, Simon, in each interaction mode, and we compare these modes against a baseline mode using only passive supervised learning. We report results from both a learning and an interaction perspective. The data show that the three modes using active learning are preferable to the mode using passive supervised learning both in terms of performance and human subject preference, but each mode has advantages and disadvantages. Based on our results, we lay out several guidelines that can inform the design of future robotic systems that use active learning in an HRI setting.},
author = {Cakmak, M. and Chao, C. and Thomaz, A.L.},
doi = {10.1109/TAMD.2010.2051030},
file = {:Users/brunohenriques/Documents/ThesisPapers/Cakmak, Chao, Thomaz/IEEE Transactions on Autonomous Mental Development/Cakmak, Chao, Thomaz - 2010 - Designing Interactions for Robot Active Learners.pdf:pdf},
isbn = {1943-0604 VO - 2},
issn = {1943-0604},
journal = {IEEE Transactions on Autonomous Mental Development},
keywords = {Active learning,human{\&}{\#}x2013,robot interaction},
number = {2},
pages = {108--118},
title = {{Designing Interactions for Robot Active Learners}},
volume = {2},
year = {2010}
}
@article{Moniz,
author = {Moniz, Helena},
file = {:Users/brunohenriques/Documents/ThesisPapers/Moniz/Unknown/Moniz - Unknown - Caracteriza{\c{c}}{\~{a}}o pros{\'{o}}dica das disflu{\^{e}}ncias e suas implica{\c{c}}{\~{o}}es para o processamento de fala Resenha.pdf:pdf},
title = {{Caracteriza{\c{c}}{\~{a}}o pros{\'{o}}dica das disflu{\^{e}}ncias e suas implica{\c{c}}{\~{o}}es para o processamento de fala Resenha}}
}
@article{Chao2010,
abstract = {This research aims to enable robots to learn from human teachers. Motivated by human social learning, we believe that a transparent learning process can help guide the human teacher to provide the most informative instruction. We believe active learning is an inherently transparent machine learning approach because the learner formulates queries to the oracle that reveal information about areas of uncertainty in the underlying model. In this work, we implement active learning on the Simon robot in the form of nonverbal gestures that query a human teacher about a demonstration within the context of a social dialogue. Our preliminary pilot study data show potential for transparency through active learning to improve the accuracy and efficiency of the teaching process. However, our data also seem to indicate possible undesirable effects from the human teacher's perspective regarding balance of the interaction. These preliminary results argue for control strategies that balance leading and following during a social learning interaction.},
author = {Chao, C. and Cakmak, M. and Thomaz, A.L.},
doi = {10.1109/HRI.2010.5453178},
file = {:Users/brunohenriques/Documents/ThesisPapers/Chao, Cakmak, Thomaz/Human-Robot Interaction (HRI), 2010 5th ACMIEEE International Conference on/Chao, Cakmak, Thomaz - 2010 - Transparent Active Learning for Robots.pdf:pdf},
isbn = {978-1-4244-4892-0},
journal = {Human-Robot Interaction (HRI), 2010 5th ACM/IEEE International Conference on},
keywords = {active learning,human-robot interaction,interactive learning,social robots,socially guided machine learning},
pages = {317--324},
title = {{Transparent Active Learning for Robots}},
year = {2010}
}
@article{Sequeira2016,
annote = {This techinique involves restriting the wizard's perceptions over the environment and the behaviors it ocntrols according to the agent's inherent limitations.
The human expert (the wizard), in order to minimize the problems adjanced to inherent to the nature of a human expert and robot perceptions it is provided to the wizard solely the information that the robot has access to.

The strategy influences the whole design lifecycle of robot in three steps:
$\backslash$begin{\{}enumerate{\}}
$\backslash$item Data Collection: Gather data from differnet interactional studies in order to gain insight of common patterns in human behavior. As the author suggests, $\backslash$textit{\{}mock-up studies{\}} in which possible human end-users of the system interact with a human expert that performs the task in place of the robot. These data allows to build task-related AI modules. These modules contitute the bulding blocks of robot's interaction strategy, i.e. modelling all the necessary perceptions and basic behaviorsfor it to pefom the task and interactiwith humans. The idea is to alleviate the wizards's work and make them focus on the revleant aspects of the robots social itneracton. Aftert WoZ study under restriected perception, the experct knowledge and the interactional data is colected in order to build the robot's controller. The data collected 
$\backslash$item Strategy extraction: In this phase, it is built a hybrid interacitonal strategy controller for the robot. In addition, the conotrollers considers strategies discovered using ML allowing identifying more ocmplex situations U(using the ML-based module). ML algorithms learn to identify which interaction behvior to trigger and when to trigger it during the interaction task using data collected during the retrived perception WoZ.
$\backslash$item Strategy refinement: Perform evaluation studies to assess the performance of the robot being autonomously controller while interacted with others within the given task. HRI reserachs are able to iteratively refine This phase allows HRI researchers to iteratively refine the robot’s behaviors for situations that may have not been properly learned or for which one could not gather enough relevant information in the previous phases.
$\backslash$end{\{}enumerate{\}}

{\%} assess... denote... usefulness... applicability of the the prposed methodonlogy, bears, beforehand, thus..., In that respect, ... accusttomed... as illustrated in FIg X., As discussed in Sec. ... endeavor

The controller is important to have hand coded rules denoting common practises employed by wizerds during $\backslash$ac{\{}WoZ{\}} studies

Case study:
To assess the usefulness and applicability of the the proposed methodologu they built a fully autonomous humaoid tutor capable of learning with young learners in a collaborative, emphatic and social manner. Thescenario is interacting between the ttor and two students in a multiplayer, turn based collaborative video game (MSCEC 11).

Realization of the proposed methodology involves dealing with practical problems :
preparation of sevaral studies
implementation of a fully functional hybrid controller form the collected data



The main purpose of data collection is to prepare and perform restricted perception WoZ studies by aquiring useful knowledge about appproparite interaction strategies in the task being considererd. Let human experts perform several interaction sessions with 

The idea is to let generated through the wizard’s learning process. Furthermore, humans that are experts on the given task to perform several Steinfeld et al. [9] make sensible differentiations between interaction sessions with prospective end-users of the system.

Mock-up models are used to conduct studies before hardware and software developemnt with the aim of abstracting the enviornment and the end users in a set of possibile minimalist scenearios [13, 14]. They used mock up studies to prepare the WoZ studies and get inspiration for the development and implmentation of tall the system components controlloing the robot's interaciton strategy. The human experts perform the desided task in place of the robot.

After aquiring expert knowledge from the mock-up studies it is possible to devise what will be the robot's perceptions and actions for the task.

Suggest using simplify the interaction aspects that are solely related with thetask itself, both in termos of perception and behavior.

The set of all perceptual and behavior machanisms as the Task AI. In terms of perceptions, the task Ai automatically yuelds a set of stae features that are accessible to the robot and are informative enough to summarize the important aspects involved in the interaciton. IN term of behaciors theuy allw hte managemnt of the robots interacitons with the humans in the desired task.


[5]- standard WoZ common used in HRI

---
perfoming restriced perception WoZ studies

Once the task AI has been implemented based on the mock up studies, we discover appropriate interaction strategies for the robot by resorting to the proposed restricted-perception WoZ techinique which.

{\%} below is cool!
Given the impracticability of manuall designing all behaviors for every predicatable situatuion that the robot might rface beforehand, one of the most effective ways of devising robot behvaiors is to learrn relevant interaction strategies given expert demonstrations


Standard woz bears some complications in later stages becausse manu perceptual and acting limitations of the robot are often disregarded by giving the wizard ocmplete access to observations over the interaciton, thus making it difficul t for the rboot to correctly interepret the environment and properly act on its own [7]. E.g., speech recognition still alcks accuracy and therefore the responses made from the robot will be mostly worse than what was expected.

To address this isues, restricted perception Woz studies by limiting what the wizard can observe from the task enviornment. In that respect, task IA provides amongs other things informative knowledge about the state of the task in the form of perceptual featuresa and a highlevel behavior repertoire. Within the methodology it corrrespondes to all the informations and behaviors available to the human expert during the WoZ studies sot that he may dinamically choose an apprproaite interaciton strategy. This will be ALL the perceptual and behavior data avialblae to later build the robot's interaciton strategy controller. 

In order to proepare the WoZ studi in the restricted perception setting, the wizard should undergo a training phase to get accusttomed to the robot's perceptual and behavior capabilities in the contexto f the desired task. THe experts feedback may also be used to iteratively refine the user interface prior to the studies.


---
Data collection
mockup sessions with high-school classerooms playing the game with observations. INterviews we also conducted to the human exeprts to undertsand their resaongin process and obtain information about the interaciton dynamics and commonn strategies. This gives insight that could be potentially triggerd by the wizard during the WoZ studies.

Task AI modules are different depedning on the specific context of the interaction task.
used CV (?) SW to detect expressive info, auditory and more task oriented information


Moreover the game-AI module adds information about critical moments (e.g. level changes)

TOgether with this perceptual information, the task AI inclduded the implementation of the robots social behaviors (i.e. all animations, gaze functions and speech, designed as a resulf ot the mock up studies). 


[17] - eELAN tool was sused to annotate gestures and gaze. [18]

We then distilled a set of social interaciton behavior for the robot to perfom withing the task that emerged during the meta-study. Such social behaviores were coded and analysed in termos of dialog dimensiosn, each provising a different interaction purpose [9]

We then perfomed a restricted perception WoZ to study possible pedagogical interaction strategies for an autonomous robotic tutor. Each session consistent of 2 students plauong with a remotely operatic robotic tutor by a restricted perception wizard guiding the students throughout their learning process.

The procuded can be sen as that of learning a policy from demonstrations, i.e., discovering a mapping between states and actions given a set of demonstrative behaviors perfomed by an expert in some task of interest [5]

Restricted perception siginificatinyl reduces the complixity in finding a correspondente between the tasks state as observed by the human and the information availbale to the robot. There is still complisity due to the implicit knowledge that the expert human wized used and is hard to directly observe form the raw data.


-- Hybrid interaction strategy controller
methodology proposes a hybrid solution ofr hte control of the robot's interaciton strategies. Involves creation of a controller where data driven ML based model and an envent driven rule based module compete for hte guidance of the robot interaction behvaiors. Each moule is designed according to the distint principle and based on different data gathered during the data collection phase.

NOtably the interaciton taks may not be well defined and as such there is nor reward function taht can be assingned that leards the robots perfomance in a desirable manner.r THe purpose of the hybrid controoler is to have a flexibile mechanism intervening at specific times depdening on the taks ans the context and a robutus system by identifying and learning form more complex situations for which tehr is no clear interaction behavior to perfomed.


---Rule base module
Module responsible for modeling well known strategins in the form of behvaior ruels, i.e. ,if perceptual state then-interaction behavior rules that are automatically triggers at specific times durning the interaction. The authors uses rules observed from the mock up studies and information gathered from the interviews.

Due to the restrived perception condition onthe exoerts. THeir decisions re more likely based on high level cofnitive knowledge about the task. These tasks related information 
to encode dooomain knowledge rulesll For example, trigger some behavior whenever some task milestone is reached or activate some attentission rule whenever it is detected that the other user is distracted form the task at hand.
{\%}---Strategy extraction phase: design principles and methods to build an interaction strategy controller for a robot based on the collected data regarding the human expert interaction streategies. Objective is to infer the decision process user by the human experts during the restricted perception WoZ studies and distill an interaction strateguy controller from them.


-- ML based module
Module responsible for automatically discovering compelx sitatuions that may arisen during the ineraciton session and for which it is hard to explicitly create behavior rules. Involves learning aninteraciton strategy given a set of demonstrations provided by human experts.

There is already modelling task specific dynamic in the rule based module. This modules at discovering complext stituaions that triggeres interaction behavior during the WoZ studies.
In their methodology they learn interaciton strategies through a mapping function beteen the robots state features and interaction behaviors given the wizerd demosntration in the restricted preception WoZ studies. These wizards have the respnsbility for chooosing which behavior and when to triggeri t. The same responsability is given to the ML module.
It starts with teh data preparation phase involving the transformation of the collected dremonstration int oa data-set of state features-bahevior pairs referred to as training instances.
The training phase learns a mapping function encoding the observed interaction strategies form the given data-set. After having learned the mapping, the module may choose an apprroate interaction behavior at run time upon request, tgiven the robots perceptual state.

---Strategy Extration in the MCEC Scenario
In EMOTE we preprocessed all the log files generated
during the interaction studies to create a set of binary state features that at some instant can either be active or inactive. In the ML-based module, the features are used to learn the interaction context upon which to activate some behavior. In the Rule-based module, they suggest that a certain behavior should be triggered whenever a specific context is active.

Regarding the Rule-based module, we analyzed data recorded from the mock-up and the sessions performed with the teachers to create pedagogical strategies that enhanced the pedagogical capabilities of the robotic tutor. For example, whenever the game started, the robot would give a short tutorial explaining the game rules, and when it finished it would wrap-up by summarizing the main achievements and analyzing the group’s performance

As for the ML-based module, we followed the procedure de-
trained the classifiers using only the behaviors performed by
picted in Fig. 4 to create a training data-set from the restricted-
the wizards. While this naturally improved the accuracy in
perceptionWoZ log files. Instead of sampling perceptual states at a contant rate, it was only recorded an instance whenever the value of some binaryt state feature changed to avoid biasing towards behavior occoruing during long, immutable states.

For states changing without the wizard's intervantion we created instances labeled with DoNothing. THe training phase, the Weka software package was used [22] to build several classification models based on ML algorithims e.g., decision trees, neural networks, clustering algorithms. We then tsted the several approaches using cross validation to obtain accuracy scores for each model.

COnsiderations:
- The data generated from the restrictez WoZ study was noisy: attributes changed very often with no wizard behavior being eperofmed and inconsisntet (same input features mapped to distint behaviors and many behaviors being triggerd in different perceptual states).
- Data was unbalenced. DoNothing was mostly 90$\backslash${\%} resulting and some behaviors had only few demonstrations resulting in low accuracy scores across the models. Notwithstanding, this is to be expected- the epcerptual processing wihin the task AI may itsleve be very noice, causing many isntances of the DoNothingto be recorded.
- In addition, the impossibility od determining the exact features that the human expercts were paying attention when triggering some behavior is naturall going to originate the obsever inconsitencies.

THe noise problem was solved by filters to clean the data from redundant feature attributes. The classifier was also only trained using the behaviors perfomed by the wizards. While this naturall imporved the acccucay in many behaviors, it led to a new problem: the trained models always tried to classify every given test instance with a known behavior, thus leading to overfitting [23] {\%}{\%}{\%}{\%}{\%}{\%}{\%}{\%}{\%}{\%}{\%} {\&}lt;{\&}lt;{\&}lt;{\&}lt;--------
This problem is more erious in runtime by making the robot behavie inapropriatley more often.

To deal with this problem it was used lazy learning, a ML technique that takes leverage of local information to discrimatne between classification lables [23]. Especifically, we developed a lazy learning technique based on an associateve metric within frequent-pattern mining [24]. For each available robot behavior, the method builds two strutuctures captuiring the context in which it should be executed and hsould not be executed. The idea is to evaluate the confidence of teach classification and learn when not to trigger incorrect incorrect behaviors.

The idea is to evaluate teh e confidence of each classification and learn when not to trigger incorrect robot behaviors. The result was am odel encoding a conservative interaction strategy that is bable to discver the behaviors perofmed during the WoZ studies that seemed more consitent and reliable.


---Refining INteraction strategies
After creating the robot's controller form the information gathered during the HRI studies. The authors validate and refine the robot's interaction through a iterative process. TO achieve that in the strategy refinement stage they use mechanisms that HRI practioners can use to refine the robots strategis, namely, thourh active learning and by means of corrective feedback. Before that, iterative evluation studies acan be perfomend to identify e.g. for which behaviors we need to improve the selection policy, or to dientify novel interaciton contexts taht did not occur during the intial studies or for which we could not appropriatly deifne a strategyu.

Regarding active learning within ML, this techinque puts on the learner the responsibility of querying an expert about specific inputs for which it needs to learn an output or imporve its accuracy [25]. In that respect, one can create methods that automatically identify areas of the robots perceptual space that were less explored during the WoZ studies. These points can beg used to actively query human experts for more detail.

Another possibility (to active learning) is to have an human expert provide corrective feeedback during the evaluation study itself, again, applying the restriceted perception condition. Rationale.: the contexts overiden by the expert are most likey the ones inccrorecly incoded or learined during the strategy extraction phase.

--Strategy Refinement in the MCEC scenario
Noted that it didn't capture some epahtetic subletis. To improve that they created a module within the task AI deteting the interaciton's emotional climate (EC) based on facial expressions features of both students leading to a different result [26]. They perfomed sessions using thinking aloud techniques [27] and identifyed points in the system that were confusing and needed refinement.

To tacle with the noicy data, it was used confidence fof all classifciations to estimate how reliable osme interacation behavior was. THen it was selected the elast releiable and asked the wizard how it could be improved.

--Evaluation
They evaluated the perrformance using a fully-autonomous hybrid controller againsta that of a restricted perception Wo and a baseline condition using a standard unrestricted WoZ.

HRI metrics:
- Adapted version of interpersonal reactivity index (IRI) [28] to measure emphaty
- godspeed series to assess the eprception of the robots santhropomorphism, animacy, likeability, perceived intelligence and perceived security [29]
- Engangement using task specific questionaire.
- TAll suing 5-point likert scale. 

Conclusions:
- Students evaluated the robotic tutor with considerable emphatetic capabilities with no significant results between conditions;
- Students preferd interacion strategies selected by the wizard in both WoZ especially in the unrestricted;
- tUey post-hoc test suggested that perception of animacy and perception of likability were statistically higher in both WoZ compared with the autonomous.
- Perceived security was statistically higher in autonomous conditions comapred wto the unrestrstricted WoZ study.

In line with expectations bevcause the interaciton decisions are still more approriate when directly performed by a human experc compared to a fully autonomus controller.
Results were not significant for the other metrics revealing positive results for the godspeed series across all studies including RPWoZ condition shwoing that the proposed methodology lead to non difeentiable impressions by the students regarding robotit tutor. High engagment was observed across all conditions

This approach takes into account the real-word constraints and limitations of agents.

key aspect: limiting what the human exppert can observe from the task to match all the perceptual features that are available to the robot. This converges the demonstrated behavior with what will be learned directly from the gatherded interacted data during the WoZ studies.

{\%}==============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

Collecting data: prepare and perform

{\%}Related work:
To the extent of our knowledge, no previous study has address a methodology to design and create interaciton strategies for a robot having into account a restricted-perception WoZ techinque.

[6] - Cont{\'{e}}m outras formas de desenhar e implementar robot interaction strategies

Knox et al. [8] proposed a model for learning interaction behaviors from human users by teaching a robot to properly behave during social interactions. Namely, the subjects are lead to believe they are teaching an autonomous robot, when in f},
author = {Sequeira, Pedro and Ribeiro, Tiago and Tullio, Eugenio Di and Petisca, Sofia and Melo, Francisco S and Paiva, Ana},
file = {:Users/brunohenriques/Documents/ThesisPapers/Sequeira et al/Proceedings of the 11th ACMIEEE International Conference on Human-Robot Interaction (HRI). in press/Sequeira et al. - 2016 - Discovering Social Interaction Strategies for Robots from Restricted-Perception Wizard-of-Oz Studies.pdf:pdf},
journal = {Proceedings of the 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI). in press},
title = {{Discovering Social Interaction Strategies for Robots from Restricted-Perception Wizard-of-Oz Studies}},
year = {2016}
}
@article{Mohammad2010,
author = {Mohammad, Y and Nishida, Toyoaki},
file = {:Users/brunohenriques/Documents/ThesisPapers/Mohammad, Nishida/RSS 2010 Workshop on Learning for Human/Mohammad, Nishida - 2010 - Unsupervised Learning of Interactive Behavior for HRI.pdf:pdf},
journal = {RSS 2010 Workshop on Learning for Human},
pages = {3--4},
title = {{Unsupervised Learning of Interactive Behavior for HRI}},
url = {http://ii.ist.i.kyoto-u.ac.jp/{~}yasser/Publications/pdf/yasser{\_}rss10.pdf},
year = {2010}
}
@article{Prepin2012,
abstract = {Gegenseitige Haltung die in einer Gruppe mit 2 Agenten entsteht. Also die Agenten beobachten sich gegenseitig und wenn das L{\"{a}}cheln richtig ausgef{\"{u}}hrt wird erkennt es der andere Agent usw. ----------------------------------------------------------------------------- When we consider communication, the "interactivenature of dialog supports interactive alignment of linguistic representations" [1]. However, verbal communication cannot be reduced to speech. Non-verbal behaviours (NVBs) of interactantsare also taking part in interactive alignment. Based on NVBs alignment, this paper proposes a model of mutual building of stance between virtual agents. In this model, we focus on the the social signal of smile. Indeed, smiles are particular non-verbal markers of agents' stances: their characteristics in terms of facial and dynamical patterns leads to discriminate stances as for instance politeness or amusement. We propose a model combining alignment of types of smile and alignement of timing of smiles (synchronisation). This model enables a dyad of agents to mutually build a common stance, to align on a shared social stance.},
author = {Prepin, Ken and Ochs, Magalie and Pelachaud, Catherine},
doi = {10.1109/SocialCom-PASSAT.2012.134},
file = {:Users/brunohenriques/Documents/ThesisPapers/Prepin, Ochs, Pelachaud/Proceedings - 2012 ASEIEEE International Conference on Privacy, Security, Risk and Trust and 2012 ASEIEEE International Conference on Social./Prepin, Ochs, Pelachaud - 2012 - Mutual Stance Building in Dyad of Virtual Agents Smile Alignment and Synchronisation.pdf:pdf},
isbn = {9780769548487},
journal = {Proceedings - 2012 ASE/IEEE International Conference on Privacy, Security, Risk and Trust and 2012 ASE/IEEE International Conference on Social Computing, SocialCom/PASSAT 2012},
keywords = {alignment,dynamical systems,smile,social interaction,stance,synchrony,virtual agent},
pages = {938--943},
title = {{Mutual Stance Building in Dyad of Virtual Agents: Smile Alignment and Synchronisation}},
year = {2012}
}
@article{Marti2006,
abstract = {Socially assistive robotics is an emerging field of research focused on assisting people through social interaction. While much attention has been paid in the past to robots that provide assistance to people through physical contact, as well as to robots that entertain through social interaction, more recently attention has been paid on socially assistive robots that mediate communication and social exchange. In the paper the argument is developed describing an exploratory study related to the use of the seal robot Paro for the treatment of dementia. The case is illuminating since it highlights the potential of social robots in supporting non pharmacological therapeutic protocols for the dementia care},
author = {Marti, Patrizia and Bacigalupo, Margherita and Giusti, Leonardo and Mennecozzi, Claudio and Shibata, Takanori},
doi = {10.1109/BIOROB.2006.1639135},
file = {:Users/brunohenriques/Documents/ThesisPapers/Marti et al/Proceedings of the First IEEERAS-EMBS International Conference on Biomedical Robotics and Biomechatronics, 2006, BioRob 2006/Marti et al. - 2006 - Socially Assistive Robotics in the Treatment of Behavioural and Psychological Symptoms of Dementia.pdf:pdf},
isbn = {1424400406},
issn = {2155-1782; 978-1-4244-0039-3},
journal = {Proceedings of the First IEEE/RAS-EMBS International Conference on Biomedical Robotics and Biomechatronics, 2006, BioRob 2006},
keywords = {Behavioural and psychological symptoms of dementia,Dementia,Socially assistive robotics,Therapeutic protocols},
number = {October},
pages = {483--488},
title = {{Socially Assistive Robotics in the Treatment of Behavioural and Psychological Symptoms of Dementia}},
volume = {2006},
year = {2006}
}
@inproceedings{Huang2010a,
abstract = {Virtual humans are embodied software agents that should not only be realistic looking but also have natural and realistic behaviors. Traditional virtual human systems learn these interaction behaviors by observing how individuals respond in face-to-face situations (i.e., direct interaction). In contrast, this paper introduces a novel methodological approach called parasocial consensus sampling (PCS) which allows multiple individuals to vicariously experience the same situation to gain insight on the typical (i.e., consensus view) of human responses in social interaction. This approach can help tease apart what is idiosyncratic from what is essential and help reveal the strength of cues that elicit social responses. Our PCS approach has several advantages over traditional methods: (1) it integrates data from multiple independent listeners interacting with the same speaker, (2) it associates probability of how likely feedback will be given over time, (3) it can be used as a prior to analyze and understand the face-to-face interaction data, (4) it facilitates much quicker and cheaper data collection. In this paper, we apply our PCS approach to learn a predictive model of listener backchannel feedback. Our experiments demonstrate that a virtual human driven by our PCS approach creates significantly more rapport and is perceived as more believable than the virtual human driven by face-to-face interaction data.},
author = {Huang, Lixing and Morency, Louis-Philippe and Gratch, Jonathan},
booktitle = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
doi = {10.1145/1838206.1838371},
file = {:Users/brunohenriques/Documents/ThesisPapers/Huang, Morency, Gratch/International Conference on Autonomous Agents and Multiagent Systems (AAMAS)/Huang, Morency, Gratch - 2010 - Parasocial Consensus Sampling Combining Multiple Perspectives to Learn Virtual Human Behavior.pdf:pdf},
isbn = {9781617387715},
issn = {15582914},
keywords = {Backchannel Feedback,Parasocial,Rapport,Virtual Humans},
pages = {10--14},
title = {{Parasocial Consensus Sampling: Combining Multiple Perspectives to Learn Virtual Human Behavior}},
url = {http://dl.acm.org/citation.cfm?id=1838371},
year = {2010}
}
@article{Schroder2010,
abstract = {This paper presents the SEMAINE API, an open source framework for building emotion-oriented systems. By encouraging and simplifying the use of standard representation formats, the framework aims to contribute to interoperability and reuse of system components in the research community. By providing a Java and C++ wrapper around a message-oriented middleware, the API makes it easy to integrate components running on di erent operating systems and written in di erent programming languages. The SEMAINE system 1.0 is presented as an example of a full-scale system built on top of the SEMAINE API. Three small example systems are described in detail to illustrate how integration between existing and new components is realised with minimal e ort.},
author = {Schr{\"{o}}der, Marc},
doi = {10.1155/2010/319406},
file = {:Users/brunohenriques/Documents/ThesisPapers/Schr{\"{o}}der/Advances in HumanComputer Interaction/Schr{\"{o}}der - 2010 - The SEMAINE API Towards a Standards-Based Framework for Building Emotion-Oriented Systems.pdf:pdf},
isbn = {1687-5893},
issn = {16875893},
journal = {Advances in HumanComputer Interaction},
pages = {1--22},
publisher = {Hindawi Publishing Corporation},
title = {{The SEMAINE API: Towards a Standards-Based Framework for Building Emotion-Oriented Systems}},
url = {http://www.hindawi.com/journals/ahci/2010/319406.html},
volume = {2010},
year = {2010}
}
@article{Andrist2014,
abstract = {Gaze aversion-the intentional redirection away from the face of an interlocutor-is an important nonverbal cue that serves a number of conversational functions, including signaling cognitive effort, regulating a conversation's intimacy level, and managing the conversational floor. In prior work, we developed a model of how gaze aversions are employed in conversation to perform these functions. In this paper, we extend the model to apply to conversational robots, enabling them to achieve some of these functions in conversations with people. We present a system that addresses the challenges of adapting human gaze aversion movements to a robot with very different affordances, such as a lack of articulated eyes. This system, implemented on the NAO platform, autonomously generates and combines three distinct types of robot head movements with different purposes: face-tracking movements to engage in mutual gaze, idle head motion to increase lifelikeness, and purposeful gaze aversions to achieve conversational functions. The results of a human-robot interaction study with 30 participants show that gaze aversions implemented with our approach are perceived as intentional, and robots can use gaze aversions to appear more thoughtful and effectively manage the conversational floor.},
author = {Andrist, Sean and Tan, Xiang Zhi and Gleicher, Michael and Mutlu, Bilge},
doi = {10.1145/2559636.2559666},
file = {:Users/brunohenriques/Documents/ThesisPapers/Andrist et al/Proceedings of the 2014 ACMIEEE International Conference on Human-robot Interaction/Andrist et al. - 2014 - Conversational Gaze Aversion for Humanlike Robots.pdf:pdf},
isbn = {978-1-4503-2658-2},
issn = {21672148},
journal = {Proceedings of the 2014 ACM/IEEE International Conference on Human-robot Interaction},
keywords = {conversation,disclosure,floor management,gaze aversion,humanlike robots,intimacy,social gaze},
pages = {25--32},
title = {{Conversational Gaze Aversion for Humanlike Robots}},
url = {http://doi.acm.org/10.1145/2559636.2559666},
year = {2014}
}
@article{Moon2000,
abstract = {This investigation examines the dynamics associated with soliciting intimate information from consumers via computers. Experiment 1 identifies two factors—reciprocity and sequence—that affect the likelihood that people will reveal intimate information about themselves via a computer. Experiment 2 provides evidence that intimate information exchanges can affect how consumers behave in subsequent interactions. Implications for marketing research and practice are discussed.},
author = {Moon, Youngme},
doi = {10.1086/209566},
isbn = {00935301},
issn = {0093-5301},
journal = {Journal of Consumer Research},
number = {4},
pages = {323--339},
pmid = {3087029},
publisher = {Oxford University Press},
title = {{Intimate Exchanges: Using Computers to Elicit Self Disclosure From Consumers}},
url = {http://www.jstor.org/stable/10.1086/209566},
volume = {26},
year = {2000}
}
@article{Fry1975,
annote = {doi: 10.1080/00224545.1975.9923275},
author = {Fry, Rick and Smith, Gene F},
doi = {10.1080/00224545.1975.9923275},
issn = {0022-4545},
journal = {The Journal of Social Psychology},
month = {jun},
number = {1},
pages = {145--146},
publisher = {Routledge},
title = {{The Effects of Feedback and Eye Contact on Performance of a Digit-Coding Task}},
url = {http://dx.doi.org/10.1080/00224545.1975.9923275},
volume = {96},
year = {1975}
}
@article{Spencer-Oatey2005,
abstract = {This paper takes rapport (Spencer-Oatey 2000, 2002) as its central con- cern, since (im)politeness is typically associated in some way with harmoni- ous/conflictual interpersonal relations. The paper discusses the factors that influence people’s dynamic perceptions of rapport, and proposes that there are three key elements: behavioral expectations, face sensitivities, and in- teractional wants. The paper explores the components of these three ele- ments and uses authentic discourse data to illustrate how people’s judg- ments about rapport can be unpackaged in relation to these elements. The approach enables us to gain a deeper understanding of the factors that influence people’s dynamic judgments of rapport, which is essential if we are to understand how and why problems of rapport occur.},
author = {Spencer-Oatey, Helen},
doi = {10.1515/jplr.2005.1.1.95},
isbn = {16125681},
issn = {1612-5681},
journal = {Politeness Research},
keywords = {Face,culture,identity,politeness,rapport,values},
month = {jan},
number = {No.1},
pages = {95--119},
publisher = {De Gruyter Mouton},
title = {{(Im)Politeness, Face and Perceptions of Rapport: Unpackaging their Bases and Interrelationships}},
url = {http://dx.doi.org/10.1515/jplr.2005.1.1.95},
volume = {1(1)},
year = {2005}
}
@inproceedings{hess1999facial,
author = {Hess, Ursula and Herrera, Pedro and Bourgeois, Patrick},
booktitle = {39th annual meeting of the Society for Psychophysiological Research, Granada, Spain},
title = {{Facial Expressions during Dyadic Interactions: The Use of EMG to Investigate the Influence of Status and Emotional State}},
year = {1999}
}
@article{Otteson1980,
abstract = {Used a repeated-measures design in 2 studies to compare the story-recall performances of 24 male and 22 female primary-school children who were read stories under 2 conditions: presence vs absence of teacher's gaze. Analysis indicated a significant positive relationship between gaze and recall, especially among males. Findings are discussed in terms of the literature on the effects of eye contact and teacher's expectancies. (21 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
annote = {doi: 10.2466/pms.1980.50.1.35},
author = {Otteson, James P. and Otteson, Carol Rodning},
doi = {10.2466/pms.1980.50.1.35},
issn = {0031-5125},
journal = {Perceptual and Motor Skills},
month = {feb},
number = {1},
pages = {35--42},
publisher = {Ammons Scientific},
title = {{Effect of Teacher's Gaze on Children's Story Recall}},
url = {http://dx.doi.org/10.2466/pms.1980.50.1.35},
volume = {50},
year = {1980}
}
@article{Sherwood1987,
abstract = {146 college students participated in 5 studies that investigated the positive effects of gaze upon recall. Various experimental designs were implemented to test the hypothesis that Ss who received gaze during an oral presentation would show better recall than Ss who did not receive gaze. In each study the experimenter delivered a verbal presentation to Ss in gaze and no-gaze conditions. Recall data support the hypothesis. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
annote = {doi: 10.2466/pms.1987.64.3c.1275},
author = {Sherwood, James V.},
doi = {10.2466/pms.1987.64.3c.1275},
issn = {1558-688X},
journal = {Perceptual and Motor Skills},
month = {jun},
number = {3, Pt 2},
pages = {1275--1278},
publisher = {Ammons Scientific},
title = {{Facilitative Effects of Gaze Upon Learning}},
url = {http://dx.doi.org/10.2466/pms.1987.64.3c.1275},
volume = {64},
year = {1987}
}
@article{Grahe1999,
abstract = {This study examined the relative impact different channels of communication had on social perception based on exposure to thin slices of the behavioral stream. Specifically, we tested the hypothesis that dyadic rapport can be perceived quickly through visual channels. Perceivers judged the rapport in 50 target interac- tions in one of five stimulus display conditions: transcript, audio, video, video + transcript, or video + audio. The data demonstrated that perceivers with access to nonverbal, visual information were the most accurate perceivers of dyadic rapport. Their judgments were found to covary with the visually encoded features that past research has linked with rapport expression. This suggests the presence of a nonverbally based implicit theory of rapport that more or less matches the natural ecology, at least as it occurs within brief samples of the behavioral stream.},
author = {Grahe, Jon E and Bernieri, Frank J},
doi = {10.1023/A:1021698725361},
file = {:Users/brunohenriques/Documents/ThesisPapers/Grahe, Bernieri/Journal of Nonverbal Behavior/Grahe, Bernieri - 1999 - The Importance of Nonverbal Cues in Judging Rapport.pdf:pdf},
isbn = {Print 0191-5886$\backslash$rElectronic 1573-3653},
issn = {01915886},
journal = {Journal of Nonverbal Behavior},
number = {4},
pages = {253--268},
title = {{The Importance of Nonverbal Cues in Judging Rapport}},
volume = {23},
year = {1999}
}
@article{Treger2013,
abstract = {Humor is a common interpersonal phenomenon that may positively influence the trajectories of social interactions. In two social interaction experiments, we examined the association between humor and liking. The first study was a secondary analysis of data from a prior experiment (originally conducted for another purpose) in which unacquainted participants engaged in a self-disclosure task and rated each other on various dimensions, including humor. In Experiment 2, unacquainted mixed-sex dyads participated in a series of either humorous or similar but non-humorous tasks. In both studies, humor was positively associated with liking and closeness; perceived reciprocal liking and enjoyment of the interaction mediated the association between humor and liking. Likewise, we found a positive association between liking and humor. Men and women did not differ in self-reported humor use. The findings suggest that humor is a mechanism used to establish connections with others across all relationships and for both sexes. Copyright © 2013 John Wiley {\&} Sons, Ltd.},
author = {Treger, Stanislav and Sprecher, Susan and Erber, Ralph},
doi = {10.1002/ejsp.1962},
file = {:Users/brunohenriques/Documents/ThesisPapers/Treger, Sprecher, Erber/European Journal of Social Psychology/Treger, Sprecher, Erber - 2013 - Laughing and Liking Exploring the Interpersonal Effects of Humor use in Initial Social Interactions.pdf:pdf},
issn = {10990992},
journal = {European Journal of Social Psychology},
number = {6},
pages = {532--543},
title = {{Laughing and Liking: Exploring the Interpersonal Effects of Humor use in Initial Social Interactions}},
volume = {43},
year = {2013}
}
@inproceedings{Kang2009,
abstract = {In this paper, we describe progress in research designed to explore the effect of the combination of avatars' visual fidelity and users' anticipated future interaction on self-disclosure in emotionally engaged and synchronous communication. We particularly aim at exploring ways to allow users' self-disclosure while securing their anonymity, even with minimal cues of a virtual human, when users anticipate future interaction. The research investigates users' self-disclosure through measuring their behaviors and feelings of social presence in several dimensions. Design and implementation of the stimulus materials and equipments are complete and data collection has begun.},
author = {Kang, Sin-Hwa and Gratch, Jonathan and Watt, James H},
booktitle = {Extended Abstracts on Human Factors in Computing Systems (CHI)},
doi = {10.1145/1520340.1520611},
file = {:Users/brunohenriques/Documents/ThesisPapers/Kang, Gratch, Watt/Extended Abstracts on Human Factors in Computing Systems (CHI)/Kang, Gratch, Watt - 2009 - The Effect of Affective Iconic Realism on Anonymous Interactants' Self-Disclosure.pdf:pdf},
isbn = {978-1-60558-247-4},
keywords = {affective behavior,anonymity,anticipated future interaction,avatar realism,contingency,embodied virtual agents,evaluation,nonverbal feedback,rapport,self-disclosure,social presence,virtual humans},
pages = {4021--4026},
title = {{The Effect of Affective Iconic Realism on Anonymous Interactants' Self-Disclosure}},
url = {http://doi.acm.org/10.1145/1520340.1520611},
year = {2009}
}
@incollection{DeKok2011,
abstract = {Computational models that attempt to predict when a virtual human should backchannel are often based on the analysis of recordings of face-to-face conversations between humans. Building a model based on a corpus brings with it the problem that people differ in the way they behave. The data provides examples of responses of a single person in a particular context but in the same context another person might not have provided a response. Vice versa, the corpus will contain contexts in which the particular listener recorded did not produce a backchannel response, where another person would have responded. Listeners can differ in the amount, the timing and the type of backchannels they provide to the speaker, because of individual differences - related to personality, gender, or culture, for instance. To gain more insight in this variation we have collected data in which we record the behaviors of three listeners interacting with one speaker. All listeners think they are having a one-on-one conversation with the speaker, while the speaker actually only sees one of the listeners. The context, in this case the speaker’s actions, is for all three listeners the same and they respond to it individually. This way we have created data on cases in which different persons show similar behaviors and cases in which they behave differently. With the recordings of this data collection study we can start building our model of backchannel behavior for virtual humans that takes into account similarities and differences between persons.},
author = {de Kok, Iwan and Heylen, Dirk},
booktitle = {Toward Autonomous, Adaptive, and Context-Aware Multimodal Interfaces. Theoretical and Practical Issues SE - 32},
doi = {10.1007/978-3-642-18184-9{\_}32},
file = {:Users/brunohenriques/Documents/ThesisPapers/de Kok, Heylen/Toward Autonomous, Adaptive, and Context-Aware Multimodal Interfaces. Theoretical and Practical Issues SE - 32/de Kok, Heylen - 2011 - The MultiLis Corpus – Dealing with Individual Differences in Nonverbal Listening Behavior.pdf:pdf},
isbn = {978-3-642-18183-2},
keywords = {Multimodal corpus,listeners,task-oriented},
pages = {362--375},
title = {{The MultiLis Corpus – Dealing with Individual Differences in Nonverbal Listening Behavior}},
url = {http://dx.doi.org/10.1007/978-3-642-18184-9{\_}32},
volume = {6456},
year = {2011}
}
@article{Bronstein2012,
abstract = {This study examined the contribution of verbal behavior to the creation of rapport in negotiation, while methodologically addressing the issue of dependence between dyadic measures, which is inherent to the concept of rapport, with the Actor-Partner Interdependence model. The approach adopted is substantially different from that of past research, which emphasized the contribution of nonverbal behavior to rapport and used averaged rapport to asses it. Drawing both from the theoretical concept of rapport and from Politeness theory, the authors developed the Verbal Rapport Assessment scale. The authors found that rapport is indeed encoded in the verbal behavior and that various verbal behaviors contribute to nego-tiators' sense of rapport, as well as to the judgment of negotiators' rapport beha-viors. Likewise, the authors found that a negotiator's sense of rapport was primarily affected by his partners' verbal behavior and by the interaction between behaviors of both sides. These findings emphasize the importance of the verbal chan-nel and the dyad in creating rapport in negotiation. Negotiation in the twenty-first century is often characterized by exclusively verbal interactions (via telephone, chat, and e-mails); negotiators from many different fields can benefit from these findings.},
author = {Bronstein, Ilan and Nelson, Noa and Livnat, Zohar and Ben-Ari, Rachel},
doi = {10.1177/0022002712448913},
file = {:Users/brunohenriques/Documents/ThesisPapers/Bronstein et al/Journal of Conflict Resolution/Bronstein et al. - 2012 - Rapport in Negotiation The Contribution of the Verbal Channel.pdf:pdf},
issn = {0022-0027},
journal = {Journal of Conflict Resolution},
keywords = {conflict resolution,negotiation,politeness theory,rapport,verbal behavior,verbal channel},
number = {6},
pages = {1089--1115},
title = {{Rapport in Negotiation: The Contribution of the Verbal Channel}},
url = {http://jcr.sagepub.com},
volume = {56},
year = {2012}
}
@article{Dimberg,
abstract = {—Studies reveal that when people are exposed to emotional facial expressions, they spontaneously react with distinct facial elec-tromyographic (EMG) reactions in emotion-relevant facial muscles. These reactions reflect, in part, a tendency to mimic the facial stimuli. We investigated whether corresponding facial reactions can be elic-ited when people are unconsciously exposed to happy and angry facial expressions. Through use of the backward-masking technique, the subjects were prevented from consciously perceiving 30-ms expo-sures of happy, neutral, and angry target faces, which immediately were followed and masked by neutral faces. Despite the fact that exposure to happy and angry faces was unconscious, the subjects reacted with distinct facial muscle reactions that corresponded to the happy and angry stimulus faces. Our results show that both positive and negative emotional reactions can be unconsciously evoked, and particularly that important aspects of emotional face-to-face commu-nication can occur on an unconscious level.},
author = {Dimberg, Ulf and Thunberg, Monika and Elmehed, Kurt},
file = {:Users/brunohenriques/Documents/ThesisPapers/Dimberg, Thunberg, Elmehed/Unknown/Dimberg, Thunberg, Elmehed - Unknown - Unconscious Facial Expressions.pdf:pdf},
title = {{Unconscious Facial Expressions}}
}
@article{Cassell2003,
author = {Cassell, Justine and Bickmore, Timothy},
file = {:Users/brunohenriques/Documents/ThesisPapers/Cassell, Bickmore/Unknown/Cassell, Bickmore - 2003 - Negotiated Collusion Modeling Social Languageand its Relationship Effects in .pdf:pdf},
keywords = {dialogue,embodied conversational agent,small talk,social interface,trust},
number = {May},
pages = {89--132},
title = {{Negotiated Collusion: Modeling Social Languageand its Relationship Effects in ...}},
year = {2003}
}
@incollection{Bickmore2012,
abstract = {We describe a computational model of user-agent relationship based on accommodation theory, in which classes of relationship are defined by the set of activities the user is willing to perform with an agent. An implementation of this model is described that uses dialogue acts as the set of relationship-defining activities, and manipulations of the model to increase user-agent intimacy over time. The implementation is integrated into a virtual agent that plays the role of an exercise counselor. Results from validation studies indicate that the implementation is successful at adapting to users’ desired intimacy level, but is not successful at increasing intimacy within the duration of the studies.},
author = {Bickmore, Timothy and Schulman, Daniel},
doi = {10.1007/978-3-642-33197-8{\_}40},
file = {:Users/brunohenriques/Documents/ThesisPapers/Bickmore, Schulman/Unknown/Bickmore, Schulman - 2012 - Empirical Validation of an Accommodation Theory-Based Model of User-Agent Relationship.pdf:pdf},
isbn = {978-3-642-33196-1},
keywords = {Computer Science},
pages = {390--403},
title = {{Empirical Validation of an Accommodation Theory-Based Model of User-Agent Relationship}},
url = {http://dx.doi.org/10.1007/978-3-642-33197-8{\_}40},
volume = {7502},
year = {2012}
}
@article{Senft,
abstract = {This paper presents a method for progres-sively increasing autonomous action selec-tion capabilities in sensitive environments, where random exploration-based learning is not desirable, using guidance provided by a human supervisor. We describe the global framework and a simulation case study based on a scenario in Robot Assisted Therapy for children with Autism Spectrum Disorder. This simulation illustrates the functional fea-tures of our proposed approach, and demon-strates how a system following these princi-ples adapts to di↵erent interaction contexts while maintaining an appropriate behaviour for the system at all times.},
author = {Senft, Emmanuel and Baxter, Paul and Belpaeme, Tony},
file = {:Users/brunohenriques/Documents/ThesisPapers/Senft, Baxter, Belpaeme/Unknown/Senft, Baxter, Belpaeme - Unknown - Human-Guided Learning of Social Action Selection for Robot-Assisted Therapy.pdf:pdf},
title = {{Human-Guided Learning of Social Action Selection for Robot-Assisted Therapy}}
}
@article{Knox2014,
annote = {Vantagens over rule-based, bayseasian network, finite state machines or evaluation function},
author = {Knox, W. Bradley and Spaulding, Samuel and Breazeal, Cynthia},
file = {:Users/brunohenriques/Documents/ThesisPapers/Knox, Spaulding, Breazeal/Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence/Knox, Spaulding, Breazeal - 2014 - Learning Social Interaction from the Wizard A Proposal.pdf:pdf},
journal = {Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence},
title = {{Learning Social Interaction from the Wizard: A Proposal}},
year = {2014}
}
@article{Kang2005,
abstract = {— This paper presents a feasibility study of using socially-aware autonomous robots to assist hospitals in reducing the effects of nursing shortages. A hands-off assistive robot is described that provides motivation and support for cardiac patients who must perform regular but painful breathing exercises. Initial validation of the system has garnered positive responses from test subjects and shows that robots have a potential to aid nursing staff in some tasks requiring patient interaction.},
author = {Kang, Kyong Il and Freedman, Sanford and Matari, Maja J and Cunningham, Mark J and Lopez, Becky},
doi = {10.1109/ICORR.2005.1501114},
file = {:Users/brunohenriques/Documents/ThesisPapers/Kang et al/9th International Conference on Rehabilitation Robotics, 2005. ICORR 2005/Kang et al. - 2005 - A Hands-Off Physical Therapy Assistance Robot for Cardiac Patients.pdf:pdf},
isbn = {0-7803-9003-2},
journal = {9th International Conference on Rehabilitation Robotics, 2005. ICORR 2005.},
pages = {337--340},
title = {{A Hands-Off Physical Therapy Assistance Robot for Cardiac Patients}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1501114$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1501114},
year = {2005}
}
@article{Scassellati2012,
abstract = {Autism spectrum disorders are a group of lifelong disabilities that affect people's ability to communicate and to understand social cues. Research into applying robots as therapy tools has shown that robots seem to improve engagement and elicit novel social behaviors from people (particularly children and teenagers) with autism. Robot therapy for autism has been explored as one of the first application domains in the field of socially assistive robotics (SAR), which aims to develop robots that assist people with special needs through social interactions. In this review, we discuss the past decade's work in SAR systems designed for autism therapy by analyzing robot design decisions, human-robot interactions, and system evaluations. We conclude by discussing challenges and future trends for this young but rapidly developing research area.},
annote = {doi: 10.1146/annurev-bioeng-071811-150036},
author = {Scassellati, Brian and {Henny Admoni} and Matari{\'{c}}, Maja},
doi = {10.1146/annurev-bioeng-071811-150036},
isbn = {978-0-8243-3514-4},
issn = {1523-9829},
journal = {Annual Review of Biomedical Engineering},
keywords = {behavioral,intelligent robots,robots for therapy,socially assistive robotics},
month = {jul},
number = {1},
pages = {275--294},
pmid = {22577778},
publisher = {Annual Reviews},
title = {{Robots for Use in Autism Research}},
url = {http://dx.doi.org/10.1146/annurev-bioeng-071811-150036},
volume = {14},
year = {2012}
}
@article{Feil-Seifer2009,
abstract = {Children with Autism Spectrum Disorders (ASD) have communication deficits and difficulties with social interaction. A lack of social behavior can hamper therapeutic interventions and can diminish the ability to learn social skills. Robots have been shown to provoke proactive social behavior in children with ASD.We are developing robot systems capable of acting as catalysts for social behavior in the context of ASD therapy. We present an experiment design for evaluating the effects of a socially assistive robot in a therapeutic setting and results of a pilot experiment with children with ASD interacting with such a robot.},
author = {Feil-Seifer, David and Matari{\'{c}}, Maja J.},
doi = {10.1007/978-3-642-00196-3{\_}24},
file = {:Users/brunohenriques/Documents/ThesisPapers/Feil-Seifer, Matari{\'{c}}/Springer Tracts in Advanced Robotics/Feil-Seifer, Matari{\'{c}} - 2009 - Toward Socially Assistive Robotics for Augmenting Interventions for Children with Autism Spectrum Disorde.pdf:pdf},
isbn = {9783642001956},
issn = {16107438},
journal = {Springer Tracts in Advanced Robotics},
pages = {201--210},
title = {{Toward Socially Assistive Robotics for Augmenting Interventions for Children with Autism Spectrum Disorders}},
volume = {54},
year = {2009}
}
@article{Fasola2012,
abstract = {In this paper, we present the design, implementation, and user study evaluation of a socially assistive robot (SAR) system designed to engage elderly users in physical exercise aimed at achieving health benefits and improving quality of life. We discuss our design methodology, which incorporates insights from psychology research in the area of intrinsic motivation, and focuses on maintaining engagement through personalized social interaction. We describe two user studies conducted to test the motivation theory in practice with our system. The first study investigated the role of praise and relational discourse in the exercise system by comparing a relational robot coach to a nonrelational robot coach. The second study evaluated participant preferences regarding user choice in the task scenario. Both studies served to evaluate the feasibility and overall effectiveness of the robot exercise system. The results of both studies are presented; they show a strong user preference for the relational over the nonrelational robot in terms of enjoyableness, companionship, and as an exercise coach, varying user preferences regarding choice, and high user ratings of the system across multiple metrics. The outcomes of the presented user studies, brought together, support the motivational capabilities of the robot, and demonstrate the viability and usefulness of the system in motivating exercise in elderly users.},
author = {Fasola, Juan and Matari{\'{c}}, Maja J.},
doi = {10.1109/JPROC.2012.2200539},
file = {:Users/brunohenriques/Documents/ThesisPapers/Fasola, Matari{\'{c}}/Proceedings of the IEEE/Fasola, Matari{\'{c}} - 2012 - Using Socially Assistive Human-Robot Interaction to Motivate Physical Exercise for Older Adults.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Exercise therapy,human-robot interaction,intrinsic motivation,quality of life technology,socially assistive robotics},
number = {8},
pages = {2512--2526},
title = {{Using Socially Assistive Human-Robot Interaction to Motivate Physical Exercise for Older Adults}},
volume = {100},
year = {2012}
}
@article{Burroughs2007,
abstract = {This study examined (1) whether or not college students in actual classrooms used resistance strategies similar to those found in earlier hypothetical-anchored research; (2) the influence of teacher immediacy on student’s differential use of those resistance strategies; and (3) the relationship among students’ willingness to comply, teachers’ nonverbal immediacy, and students’ compliance resistance behaviors with perceived cognitive and affective learning. Based on both qualitative and quantitative data, college students (N0564) reported limited resistance attempts and strategies. They also reported greater willingness to comply with immediate as opposed to nonimmediate teachers, and their willingness to comply was related to cognitive and affective learning. Findings in this study suggest that teachers’ nonverbal immediacy is fundamental to classroom management and learning.},
author = {Burroughs, Nancy F},
doi = {10.1080/03634520701530896},
isbn = {0363-4523},
issn = {0363-4523},
journal = {Communication Education},
keywords = {affective learning,cognitive learning,student compliance,student resistance,teacher inmediacy},
month = {oct},
number = {4},
pages = {453--475},
publisher = {Routledge},
title = {{A Reinvestigation of the Relationship of Teacher Nonverbal Immediacy and Student Compliance-Resistance with Learning}},
url = {http://dx.doi.org/10.1080/03634520701530896 http://www.informaworld.com/openurl?genre=article{\&}doi=10.1080/03634520701530896{\&}magic=crossref||D404A21C5BB053405B1A640AFFD44AE3},
volume = {56},
year = {2007}
}
@phdthesis{Andersen1979,
author = {Andersen, Janis F},
editor = {Null},
isbn = {null},
pages = {null},
series = {null},
title = {{The Relationship between Teacher Immediacy and Teaching Effectiveness}},
volume = {null},
year = {1979}
}
@article{Bickmore2005,
abstract = {This study examines the acceptance and usability of an animated conversational agent designed to establish long- term relationships with older, mostly minority adult users living in urban neighborhoods. The agent plays the role of an exercise advisor who interacts with subjects daily for two months on a touch-screen computer installed in their homes for the study. Survey results indicate the eight subjects who completed the pilot study (aged 62-82) found the agent very easy to interact with, even though most of them had little or no previous experience using computers. Most subjects also indicated strong liking for and trust in the agent, felt that their relationship with the agent was more similar to a close friend than a stranger, and expressed a strong desire to continue working with the agent at the end of the study. These results were also confirmed through qualitative analysis of post-experiment debrief transcripts.},
author = {Bickmore, Timothy W. and Caruso, Lisa and Clough-Gorr, Kerri},
doi = {10.1145/1056808.1056879},
file = {:Users/brunohenriques/Documents/ThesisPapers/Bickmore, Caruso, Clough-Gorr/CHI '05 extended abstracts on Human factors in computing systems - CHI '05/Bickmore, Caruso, Clough-Gorr - 2005 - Acceptance and Usability of a Relational Agent Interface by Urban Older Adults.pdf:pdf},
isbn = {1595930027},
journal = {CHI '05 extended abstracts on Human factors in computing systems - CHI '05},
pages = {1212},
title = {{Acceptance and Usability of a Relational Agent Interface by Urban Older Adults}},
year = {2005}
}
@article{Bull1981,
abstract = {A number of studies have examined in the laboratory the effects of an individual's eye-gaze upon the behavior of another. In this study the effects of gaze were investigated in a real-life setting in which a collector of money for a charity either looked a possible donor in the eye when asking for money or looked at the collecting tin. Significantly more money was donated in the former condition. While neither the style of dress of the collector nor the locality in which the collections were made had an overall effect, significant interactive effects were noted for gaze and style of dress, for style of dress and locality, and for gaze and locality. Gaze was a more potent factor when the collector was dressed casually than smartly, and when the collections were made in high-rise flats as opposed to terraced houses.},
address = {US},
author = {Bull, Ray},
doi = {10.1177/001872678103401005},
isbn = {1741-282X(Electronic);0018-7267(Print)},
issn = {0018-7267},
journal = {Human Relations},
keywords = {*Charitable Behavior,*Environment,*Eye Contact,Physical Appearance},
number = {10},
pages = {895--905},
publisher = {Sage Publications},
title = {{The Influences of Eye-Gaze, Style of Dress, and Locality on the Amounts of Money Donated to a Charity}},
volume = {34},
year = {1981}
}
@article{Tickle-Degnen1990,
abstract = {The purpose of this article is to offer a conceptualization of rapport that has utilityfor identifiing the nonverbal correlates associated with rapport. W e describe the nature of rapport in terms of a dynamic structure of three interrelating components: mutual attentiveness, positivity, and coor- dination. W epropose that the relative weighting of these components in the experience of rapport changes over the course of a developing relationship between individuals. In early interactions, positivity and attentiveness are more heavily weighted than coordination, whereas in later interactions, coordination and attentiveness are the more heavily weighted components. Because of the gestalt nature of the experience of rapport, it is not easy to identifi nonverbal behavioral correlates of the components. W e discuss two approaches to nonverbal measurement, molecular and molar , along with recommendations for their appropriate application in the study of rapport at different stages of an interpersonal relationship. W epresent a meta-analytic study that demon- strates the effect of nonverbal behavior, measured at the molecular level, on the positivity component of rapport, and we conclude with an outline of hypotheses relevant to the investigation of the nonverbal correlates of rapport.},
author = {Tickle-Degnen, Linda and Rosenthal, Robert},
doi = {10.1207/s15327965pli0104{\_}1},
file = {:Users/brunohenriques/Documents/ThesisPapers/Tickle-Degnen, Rosenthal/Psychological Inquiry/Tickle-Degnen, Rosenthal - 1990 - The Nature of Rapport and Its Nonverbal Correlates.pdf:pdf},
isbn = {1047-840X},
issn = {1047-840X},
journal = {Psychological Inquiry},
number = {4},
pages = {285--293},
pmid = {7400892},
title = {{The Nature of Rapport and Its Nonverbal Correlates}},
volume = {1},
year = {1990}
}
@article{Burns1984,
abstract = {Notes that the basic aspects of childcare are often overlooked or underestimated and that the manner in which childcare workers approach each child as a separate and unique human being is a critical aspect of professional and humane practice. Information useful in helping childcare workers develop basic rapport and solid relations with children is presented. Techniques for establishing rapport (e.g., reflecting, language, physical contact) are described. Areas of awareness essential to therapeutic effectiveness are delineated. Case presentations that highlight effective strategies and the common mistakes of caregivers are included. (5 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
address = {Canada},
author = {Burns, Michael},
journal = {Journal of Child Care},
keywords = {*Child Care Workers,Interpersonal Attraction},
number = {2},
pages = {47--57},
publisher = {Journal of Child and Youth Care},
title = {{Rapport and Relationships: The Basis of Child Care}},
volume = {2},
year = {1984}
}
@article{Drolet2000,
abstract = {We propose that face-to-face contact fosters the development of rapport and thereby helps negotiators coordinate on mutually beneficial settlements in mixed-motive conflicts. Specifically, we investigate whether, in a cooperative climate, negotiators' visual access to each other's nonverbal behavior fosters a dyadic state of rapport that facilitates mutual cooperation. Experiment 1 manipulated whether negotiators stood face-to-face or side-by-side (unable to see each other) in a simulated strike negotiation. Face-to-face dyads were more likely to coordinate on a settlement early in the strike, resulting in higher joint gains. An alternative interpretation in terms of an anticipatory effect of face-to-face contact was not supported. Experiment 2 manipulated whether previously unacquainted negotiators conversed face-to-face or by telephone before separating to play a conflict game with the structure of a Prisoner's Dilemma game. Face-to-face dyads were more likely to coordinate on high joint gain outcomes. The facilitatory effect of face-to-face contact was statistically mediated by a measure of dyadic rapport. Results did not support alternative interpretations based on individual-level positive affect or expectations about opponents. We conclude with a discussion of the role of affective and dyad-level processes in social psychological models of conflict resolution.},
author = {Drolet, Aimee L. and Morris, Michael W.},
doi = {10.1006/jesp.1999.1395},
file = {:Users/brunohenriques/Documents/ThesisPapers/Drolet, Morris/Journal of Experimental Social Psychology/Drolet, Morris - 2000 - Rapport in Conflict Resolution Accounting for How Face-to-Face Contact Fosters Mutual Cooperation in Mixed-Motiv.pdf:pdf},
isbn = {0022-1031},
issn = {00221031},
journal = {Journal of Experimental Social Psychology},
pages = {26--50},
title = {{Rapport in Conflict Resolution: Accounting for How Face-to-Face Contact Fosters Mutual Cooperation in Mixed-Motive Conflicts}},
url = {http://www.sciencedirect.com/science/article/pii/S0022103199913951},
volume = {36},
year = {2000}
}
@article{Tullio2015,
author = {Ribeiro, Tiago and Pereira, Andr{\'{e}} and Tullio, Eug{\'{e}}nio Di and Paiva, Ana},
file = {:Users/brunohenriques/Documents/ThesisPapers/Ribeiro et al/AAAI 2016 Spring Symposium on ”Enabling Computing Research in Socially Intelligent Human-Robot Interaction A Community Driven Mo. in press/Ribeiro et al. - 2016 - The SERA Ecosystem Socially Expressive Robotics Architecture.pdf:pdf},
journal = {AAAI 2016 Spring Symposium on ”Enabling Computing Research in Socially Intelligent Human-Robot Interaction: A Community Driven Modular Research Platform. in press},
title = {{The SERA Ecosystem : Socially Expressive Robotics Architecture}},
year = {2016}
}
@article{Visser2014,
author = {Visser, Thomas and Traum, David and DeVault, David and op den Akker, Rieks},
doi = {10.1007/s12193-013-0147-7},
file = {:Users/brunohenriques/Documents/ThesisPapers/Visser et al/Journal on Multimodal User Interfaces/Visser et al. - 2014 - Toward a Model for Incremental Grounding in Spoken Dialogue Systems.pdf:pdf},
isbn = {1783-7677},
issn = {17838738},
journal = {Journal on Multimodal User Interfaces},
keywords = {Grounding,Incremental language processing,Spoken dialogue systems},
number = {1},
pages = {61--73},
title = {{Toward a Model for Incremental Grounding in Spoken Dialogue Systems}},
volume = {8},
year = {2014}
}
@article{Cassell2007,
abstract = {We investigate the role of increasing friendship in dialogue, and propose a first step towards a computational model of the role of long-term relationships in language use between humans and embodied conver- sational agents. Data came from a study of friends and strangers, who either could or could not see one another, and who were asked to give directions to one-another, three subsequent times. Analysis focused on differences in the use of dialogue acts and non-verbal behaviors, as well as co- occurrences of dialogue acts, eye gaze and head nods, and found a pattern of verbal and nonverbal behavior that differentiates the dialogue of friends from that of strang- ers, and differentiates early acquaintances from those who have worked together be- fore. Based on these results, we present a model of deepening rapport which would enable an ECA to begin to model patterns of human relationships.},
annote = {long term rapport

evolution from strangers to freisnds and how that affects rapport.

Okay is given

coordination related stuff},
author = {Cassell, Justine and Gill, Alastair and Tepper, Paul},
doi = {10.3115/1610065.1610071},
file = {:Users/brunohenriques/Documents/ThesisPapers/Cassell, Gill, Tepper/Workshop on Embodied Language Processing/Cassell, Gill, Tepper - 2007 - Coordination in Conversation and Rapport.pdf:pdf},
journal = {Workshop on Embodied Language Processing},
pages = {41--50},
title = {{Coordination in Conversation and Rapport}},
year = {2007}
}
@article{Nadler2003,
author = {Nadler, Janice},
file = {:Users/brunohenriques/Documents/ThesisPapers/Nadler/Marq. L. Rev/Nadler - 2003 - Rapport in Negotiation and Conflict Resolution.pdf:pdf},
journal = {Marq. L. Rev.},
number = {1990},
pages = {875--882},
title = {{Rapport in Negotiation and Conflict Resolution}},
url = {http://heinonlinebackup.com/hol-cgi-bin/get{\_}pdf.cgi?handle=hein.journals/marqlr87{\&}section=52},
volume = {285},
year = {2003}
}
@article{Karacora2012,
abstract = {he purpose of the present research is to investigate whether virtual agents can help enhance participants' performance, effort and motivation in mathematics. We hypothesize that a minimal amount behavioral realism induced by display of rapport is necessary for any social effects to occur in human-computer interaction. Further, we examine whether social facilitation effects occur depending on the gender of the participants and the interacting virtual agents. In a 2x2 between subjects design, participants interacted with a male or female virtual agent that either displayed rapport or no rapport. Our results confirm that gender plays a role when interacting with virtual agents that are capable of establishing rapport. Participants' performance and effort were significantly enhanced when interacting with an agent of opposite gender that displayed rapport. Our results have implications on designing agents for education and training purposes.},
annote = {Participants’ performance and effort were significantly enhanced when interacting with an agent of opposite gender that displayed rapport. Our results have significantly enhanced when interacting with an agent of opposite gender that displayed rapport.

Rapport has been shown as an effective way to create
behavioral realism in virtual agents. In social psychology, rapport is described as the establishment of a positive relationship among interaction partners by rapidly detecting and responding to each other’s nonverbal behavior (Gratch et al., 2007a). This


To produce listening behaviors, the
Rapport Agent first collects and analyzes audiovisual
features from the speaker’s voice (silence, speech) and
upper-body movements (head nod, smile, eye gaze) in real
time.},
author = {Karacora, Bilge},
file = {:Users/brunohenriques/Documents/ThesisPapers/Karacora/Proceedings of the {\ldots}/Karacora - 2012 - The Influence of Virtual Agents' Gender and Rapport on Enhancing Math Performance.pdf:pdf},
journal = {Proceedings of the {\ldots}},
keywords = {rapport,social facilitation,stem,virtual agents},
number = {2007},
pages = {563--568},
title = {{The Influence of Virtual Agents' Gender and Rapport on Enhancing Math Performance}},
url = {http://mindmodeling.org/cogsci2012/papers/0108/paper0108.pdf$\backslash$nhttp://ict.usc.edu/pubs/The Influence of Virtual Agents Gender and Rapport on Enhancing Math Performance.pdf},
volume = {1},
year = {2012}
}
@incollection{Zhao2014,
abstract = {Rapport has been identified as an important function of hu- man interaction, but to our knowledge no model exists of building and maintaining rapport between humans and conversational agents over the long-term that operates at the level of the dyad. In this paper we lever- age existing literature and a corpus of peer tutoring data to develop a framework able to explain how humans in dyadic interactions build, maintain, and destroy rapport through the use of specific conversational strategies that function to fulfill specific social goals, and that are instan- tiated in particular verbal and nonverbal behaviors. We demonstrate its functionality using examples from our experimental data.},
author = {Zhao, Ran and Papangelis, Alexandros and Cassell, Justine},
booktitle = {Intelligent Virtual Agents},
file = {:Users/brunohenriques/Documents/ThesisPapers/Zhao, Papangelis, Cassell/Intelligent Virtual Agents/Zhao, Papangelis, Cassell - 2014 - Towards a Dyadic Computational Model of Rapport Management for Human-Virtual Agent Interaction.pdf:pdf},
isbn = {978-3-319-09766-4, 978-3-319-09767-1},
pages = {514--527},
title = {{Towards a Dyadic Computational Model of Rapport Management for Human-Virtual Agent Interaction}},
year = {2014}
}
@incollection{Papangelis2014,
abstract = {Rapport has been identified as an important factor in human task performance. Motivated by the proliferation of virtual agents that assist humans on various tasks, we propose a computational architecture for virtual agents, building on our own work on a dyadic model of rapport between humans and virtual agents. We show how such a system can be trained in order to build, maintain and destroy rapport.},
annote = {so important},
author = {Papangelis, Alexandros and Zhao, Ran and Cassell, Justine},
booktitle = {Intelligent Virtual Agents},
file = {:Users/brunohenriques/Documents/ThesisPapers/Papangelis, Zhao, Cassell/Intelligent Virtual Agents/Papangelis, Zhao, Cassell - 2014 - Towards a Computational Architecture of Dyadic Rapport Management for Virtual Agents.pdf:pdf},
isbn = {978-3-319-09766-4, 978-3-319-09767-1},
pages = {320--324},
title = {{Towards a Computational Architecture of Dyadic Rapport Management for Virtual Agents}},
year = {2014}
}
@inproceedings{Stanton2014,
annote = {notas importantes sobre i impacto das cenas},
author = {Stanton, Christopher and Stevens, Catherine J},
booktitle = {Social Robotics},
file = {:Users/brunohenriques/Documents/ThesisPapers/Stanton, Stevens/Social Robotics/Stanton, Stevens - 2014 - Robot Pressure The Impact of Robot Eye Gaze and Lifelike Bodily Movements upon Decision-Making and Trust.pdf:pdf},
keywords = {compliance,eye gaze,human-robot interaction,nonverbal communication,persuasion,trust},
pages = {330--339},
title = {{Robot Pressure : The Impact of Robot Eye Gaze and Lifelike Bodily Movements upon Decision-Making and Trust}},
year = {2014}
}
@article{Nomura2015,
author = {Nomura, Tatsuya and Kanda, Takayuki},
doi = {10.13140/RG.2.1.3452.5602},
file = {:Users/brunohenriques/Documents/ThesisPapers/Nomura, Kanda/Unknown/Nomura, Kanda - 2015 - Who Expect Rapport with Robots A Survey-Based Study for Analysis of People’s Expectation.pdf:pdf},
isbn = {8177544713},
number = {MAY},
title = {{Who Expect Rapport with Robots? A Survey-Based Study for Analysis of People’s Expectation}},
year = {2015}
}
@article{Nomura2014,
annote = {already read this....

Buttler was more expected to talk more.... and the business also but tthe one on public working area not.

Applied only in Japan so cultural differences were not taken into account},
author = {Nomura, Tatsuya},
doi = {10.1145/2658861.2658869},
file = {:Users/brunohenriques/Documents/ThesisPapers/Nomura/Unknown/Nomura - 2014 - Differences of Expectation of Rapport with Robots Dependent on Situations.pdf:pdf},
isbn = {9781450330350},
pages = {383--389},
title = {{Differences of Expectation of Rapport with Robots Dependent on Situations}},
year = {2014}
}
@article{Dautenhahn2003,
abstract = {This paper discusses robots that are operational within a human-inhabited environment. Specifically, we identify different roles that such robots can adopt, reflecting different human-robot relationships. We discuss three different roles of robots in a project where we develop a robot as a therapeutic tool for children with autism: the robot as a therapeutic playmate, the robot as a social mediator, and the robot as a model social agent. Implications of these roles that go beyond this particular project are discussed.},
author = {Dautenhahn, K.},
doi = {10.1017/S0263574703004922},
isbn = {0263-5747},
issn = {02635747},
journal = {Robotica},
keywords = {Computer Science},
number = {4},
pages = {443--452},
title = {{Roles and Functions of Robots in Human Society-Implications from Research in Autism Therapy}},
url = {http://hdl.handle.net/2299/193},
volume = {21},
year = {2003}
}
@article{Klemmer2006,
abstract = {Our physical bodies play a central role in shaping human experience in the world, understandingof the world, and interactions in the world. This paper draws on theories of embodiment - from psychology, sociology, and philosophy},
author = {Klemmer, Scott R and Hartmann, Bj{\"{o}}rn and Takayama, Leila},
doi = {10.1145/1142405.1142429},
isbn = {1595933670},
journal = {DIS '06: Proceedings of the 6th conference on Designing Interactive systems},
keywords = {Embodiment,bodies,embodied interaction,ubiquito},
pages = {140--149},
title = {{How Bodies Matter: Five Themes for Interaction Design}},
url = {http://portal.acm.org/citation.cfm?doid=1142405.1142429$\backslash$npapers3://publication/doi/10.1145/1142405.1142429},
year = {2006}
}
@article{Fong2003,
abstract = {This paper reviews 'socially interactive robots': robots for which social human-robot interaction is important. We begin by discussing the context for socially interactive robots, emphasizing the relationship to other research fields and the different forms of 'social robots'. We then present a taxonomy of design methods and system components used to build socially interactive robots. Finally, we describe the impact of these robots on humans and discuss open issues. An expanded version of this paper, which contains a survey and taxonomy of current applications, is available as a technical report [T. Fong, I. Nourbakhsh, K. Dautenhahn, A survey of socially interactive robots: concepts, design and applications, Technical Report No. CMU-RI-TR-02-29, Robotics Institute, Carnegie Mellon University, 2002]},
author = {Fong, T and Nourbakhsh, I and Dautenhahn, K},
doi = {10.1016/S0921-8890(02)00372-X},
file = {:Users/brunohenriques/Documents/ThesisPapers/Fong, Nourbakhsh, Dautenhahn/Robotics and Autonomous Systems/Fong, Nourbakhsh, Dautenhahn - 2003 - A Survey of Socially Interactive Robots.pdf:pdf},
isbn = {09218890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {human,interaction aware robot,robot interaction,sociable robot,social robot,socially interactive robot},
number = {3-4},
pages = {143--166},
pmid = {15717012},
title = {{A Survey of Socially Interactive Robots}},
url = {file://localhost/Users/juan/Trabajo/Dropbox/Papers/2003{\_}Fong{\_}A survey of socially interactive.pdf},
volume = {42},
year = {2003}
}
@article{Trafton2013,
abstract = {We presentACT-R/E (Adaptive Character of Thought-Rational / Embodied), a cognitive architecture for human-robot interaction. Our reason for using ACT-R/E is two-fold. First, ACT-R/E enables researchers to build good embodied models of people to understand how and why people think the way they do. Then, we leverage that knowledge of people by using it to predict what a person will do in different situations; e.g., that a person may forget something and may need to be reminded or that a person cannot see everything the robot sees. We also discuss methods of how to evaluate a cognitive architecture and show numerous empirically validated examples of ACT-R/E models.},
author = {Trafton, Greg and Hiatt, Laura and Harrison, Anthony and Tanborello, Frank and Khemlani, Sangeet and Schultz, Alan},
doi = {10.5898/JHRI.2.1.Trafton},
issn = {21630364},
journal = {Journal of Human-Robot Interaction},
keywords = {cognitive architectures,cognitive modeling,human-robot interaction},
number = {1},
pages = {30--55},
title = {{ACT-R/E: An Embodied Cognitive Architecture for Human-Robot Interaction}},
url = {http://www.humanrobotinteraction.org/journal/index.php/HRI/article/view/121},
volume = {2},
year = {2013}
}
@article{Kahn2008,
abstract = {We propose that Christopher Alexander’s idea of design patterns can benefit the emerging field of HRI. We first discuss four features of design patterns that appear particularly useful. For example, a pattern should be specified abstractly enough such that many different instantiations of the pattern can be uniquely realized in the solution to specific problems in context. Then, after describing our method for generating patterns, we offer and describe eight possible design patterns for sociality in human robot interaction: initial introduction, didactic communication, in motion together, personal interests and history, recovering from mistakes, reciprocal turn-taking in game context, physical intimacy, and claiming unfair treatment or wrongful harms. We also discuss the issue of validation of design patterns. If a design pattern program proves successful, it will provide HRI researchers with basic knowledge about human robot interaction, and save time through the reuse of patterns to achieve high levels of sociality.},
author = {Kahn, Peter H and Freier, Nathan G and Kanda, Takayuki and Ishiguro, Hiroshi and Ruckert, Jolina H and Severson, Rachel L and Kane, Shaun K},
doi = {10.1145/1349822.1349836},
file = {:Users/brunohenriques/Documents/ThesisPapers/Kahn et al/Proceedings of the 3rd international conference on Human robot interaction - HRI '08/Kahn et al. - 2008 - Design Patterns for Sociality in Human-Robot Interaction.pdf:pdf},
isbn = {9781605580173},
issn = {9781605580173},
journal = {Proceedings of the 3rd international conference on Human robot interaction - HRI '08},
pages = {97},
title = {{Design Patterns for Sociality in Human-Robot Interaction}},
url = {http://portal.acm.org/citation.cfm?doid=1349822.1349836},
year = {2008}
}
@article{Kiesler2008,
abstract = {People's physical embodiment and presence increase their salience and importance. We predicted people would anthropomorphize an embodied humanoid robot more than a robot-like agent, and a collocated more than a remote robot. A robot or robot-like agent interviewed participants about their health. Participants were either present with the robot/agent, or interacted remotely with the robot/agent projected life—size on a screen. Participants were more engaged, disclosed less undesirable behavior, and forgot more with the robot versus the agent. They ate less and anthropomorphized most with the collocated robot. Participants interacted socially and attempted conversational grounding with the robot/agent though aware it was a machine. Basic questions remain about how people resolve the ambiguity of interacting with a humanlike nonhuman.},
author = {Kiesler, Sara and Powers, Aaron and Fussell, Susan R. and Torrey, Cristen},
doi = {10.1521/soco.2008.26.2.169},
isbn = {0278016X},
issn = {0278-016X},
journal = {Social Cognition},
number = {2},
pages = {169--181},
pmid = {19058991},
title = {{Anthropomorphic Interactions with a Robot and Robot–like Agent}},
volume = {26},
year = {2008}
}
@article{Malfaz2011,
abstract = {Lately, lots of effort has been put into the construction of robots able to live among humans. This fact has favored the development of personal or social robots, which are expected to behave in a natural way. This implies that these robots could meet certain requirements, for example, to be able to decide their own actions (autonomy), to be able to make deliberative plans (reasoning), or to be able to have an emotional behavior in order to facilitate human{\&}{\#}x2013;robot interaction. In this paper, the authors present a bioinspired control architecture for an autonomous and social robot, which tries to accomplish some of these features. In order to develop this new architecture, authors have used as a base a prior hybrid control architecture (AD) that is also biologically inspired. Nevertheless, in the later, the task to be accomplished at each moment is determined by a fix sequence processed by the Main Sequencer. Therefore, the main sequencer of the architecture coordinates the previously programmed sequence of skills that must be executed. In the new architecture, the main sequencer is substituted by a decision making system based on drives, motivations, emotions, and self-learning, which decides the proper action at every moment according to robot's state. Consequently, the robot improves its autonomy since the added decision making system will determine the goal and consequently the skills to be executed. A basic version of this new architecture has been implemented on a real robotic platform. Some experiments are shown at the end of the paper.},
author = {Malfaz, Mar{\'{\i}}a and Castro-Gonz{\'{a}}lez, {\'{A}}lvaro and Barber, Ram{\'{o}}n and Salichs, Miguel a.},
doi = {10.1109/TAMD.2011.2112766},
file = {:Users/brunohenriques/Documents/ThesisPapers/Malfaz et al/IEEE Transactions on Autonomous Mental Development/Malfaz et al. - 2011 - A Biologically Inspired Architecture for an Autonomous and Social Robot.pdf:pdf},
isbn = {1943-0604 VO - PP},
issn = {19430604},
journal = {IEEE Transactions on Autonomous Mental Development},
keywords = {Autonomy,cognitive robotics,control architectures,decision making systems,emotions,motivations},
number = {3},
pages = {232--246},
title = {{A Biologically Inspired Architecture for an Autonomous and Social Robot}},
volume = {3},
year = {2011}
}
@article{Goodrich2007,
abstract = {Human-Robot Interaction (HRI) has recently received considerable attention in the academic community, in labs, in technology companies, and through the media. Because of this attention, it is desirable to present a survey of HRI to serve as a tutorial to people outside the field and to promote discussion of a unified vision of HRI within the field. The goal of this review is to present a unified treatment of HRI-related problems, to identify key themes, and discuss challenge problems that are likely to shape the field in the near future. Although the review follows a survey structure, the goal of presenting a coherent “story” of HRI means that there are necessarily some well-written, intriguing, and influential papers that are not referenced. Instead of trying to survey every paper, we describe the HRI story from multiple perspectives with an eye toward identifying themes that cross applications. The survey attempts to include papers that represent a fair cross section of the universities, government efforts, industry labs, and countries that contribute to HRI, and a cross section of the disciplines that contribute to the field, such as human, factors, robotics, cognitive psychology, and design.},
author = {Goodrich, Michael A and Schultz, Alan C},
doi = {10.1561/1100000005},
isbn = {9781601980922},
issn = {1551-3955},
journal = {Foundations and Trends® in Human-Computer Interaction},
number = {3},
pages = {203--275},
title = {{Human-Robot Interaction: A Survey}},
volume = {1},
year = {2007}
}
@article{Huang2012,
abstract = {Social interaction involves a large number of patterned behaviors that people employ to achieve particular communicative goals. To achieve fluent and effective humanlike communication, robots must seamlessly integrate the necessary social behaviors for a given interaction context. However, very little is known about how robots might be equipped with a collection of such behaviors and how they might employ these behaviors in social interaction. In this paper, we propose a framework that guides the generation of social behavior for humanlike robots by systematically using specifications of social behavior from the social sciences and contextualizing these specifications in an Activity-Theory-based interaction model. We present the Robot Behavior Toolkit, an open-source implementation of this framework as a Robot Operating System (ROS) module and a community-based repository for behavioral specifications, and an evaluation of the effectiveness of the Toolkit in using these specifications to generate social behavior in a human-robot interaction study, focusing particularly on gaze behavior. The results show that specifications from this knowledge base enabled the Toolkit to achieve positive social, cognitive, and task outcomes, such as improved information recall, collaborative work, and perceptions of the robot. © 2012 ACM.},
annote = {o que {\'{e}} activity-theroy interaction model? esta explicado. ver notas},
author = {Huang, Cm and Mutlu, Bilge},
doi = {10.1145/2157689.2157694},
file = {:Users/brunohenriques/Documents/ThesisPapers/Huang et al/Robotics and Autonomous Systems/Huang et al. - 2003 - Seven Principles of Efficient Human Robot Interaction.pdf:pdf},
isbn = {9781450310635},
issn = {2167-2121},
journal = {Proceedings of the Seventh Annual ACM/IEEE international conference on Human-Robot Interaction (HRI '12)},
keywords = {Activity Theory,Robot Behavior Toolkit,generating social behavior,human-robot interaction},
number = {September},
pages = {25--32},
title = {{Robot Behavior Toolkit: Generating Effective Social Behaviors for Robots}},
url = {http://dl.acm.org/citation.cfm?doid=2157689.2157694 http://dl.acm.org/citation.cfm?id=2157694},
year = {2012}
}
@article{Andrist2015,
annote = {Sean Andrist et al., have shown evidence that creating agent that match the partner's personality in engagement in repetitives tasks $\backslash$cite{\{}Andrist2015{\}}.

Demonstrate the importance of taking the user's intrisic motivaiton into account when attempting to motiveate and increase complicante.

In this paper, we investigate how robots may achieve the first form of adaptation and present the design and evaluation of gaze behaviors for socially assistive robots that enable the robot to match the personality of the user, thereby more effec- tively motivating users to repeatedly engage in a therapeutic task.

we demonstrate the positive effect of personality matching on a user’s motivation to engage in a repetitive task

we presented two models of gaze behavior that can exhibit either an extroverted or introverted personality.

and nonverbal behavior are not always linked in simple ways [13]. Personality can be expressed differently in different contexts, group compositions, cultures, and com- binations, and this richness should be taken into account in future work to align robot behaviors with user personalities.

A large amount of previous research has empirically demon-strated the positive effect of gaze on compliance.

must take into account a per- son’s motivation, which can vary not only in magnitude but also in orientation [38]

separation of two phases withing tasks: in-task phase (actual exection) and between-task (time between tasks when the terapists must provide encouragement to persist with the task)

In both personalities, more gaze is towarded to the user between task-phase (which invevoes monitoring the users action)},
author = {Andrist, Sean and Mutlu, Bilge and Tapus, Adriana},
doi = {10.1145/2702123.2702592},
file = {:Users/brunohenriques/Documents/ThesisPapers/Andrist, Mutlu, Tapus/Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems - CHI '15/Andrist, Mutlu, Tapus - 2015 - Look Like Me Matching Robot Personality via Gaze to Increase Motivation.pdf:pdf},
isbn = {9781450331456},
journal = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems - CHI '15},
keywords = {compliance,gaze,human-robot interaction (hri),motivation,personality,similarity-attraction},
number = {1},
pages = {3603--3612},
title = {{Look Like Me: Matching Robot Personality via Gaze to Increase Motivation}},
url = {http://dl.acm.org/citation.cfm?id=2702123.2702592},
volume = {2},
year = {2015}
}
@article{Lin2012,
abstract = {In this paper, we seek to review the broad landscape of research in computational emotions and cognition. We begin by classifying and organizing an enumeration of recent models and systems and then discuss some of the landmark models from the literature, such as EMA andWASABI.We then discuss open problems with the current state of research. These issues are standardizing criteria for evaluation of models, the complexity and breadth of the domain, and the need to implement a working system which addresses integration with more of the rich history of AI research. We also provide suggestions for future research, particularly standardization to facilitate community collaboration.},
author = {Lin, Jerry and Spraragen, Marc and Zyda, Michael},
file = {:Users/brunohenriques/Documents/ThesisPapers/Lin, Spraragen, Zyda/Advances in Cognitive Systems/Lin, Spraragen, Zyda - 2012 - Computational Models of Emotion and Cognition.pdf:pdf},
journal = {Advances in Cognitive Systems},
pages = {59--76},
title = {{Computational Models of Emotion and Cognition}},
url = {http://cogsys.org/pdf/paper-3-2-39.pdf},
volume = {2},
year = {2012}
}
@incollection{Buschmeier2011,
abstract = {Rapport, the feeling of being “in sync” with your conversational partners, is argued to underlie many desirable social effects. By generating proper verbal and nonverbal behaviors, virtual humans have been seen to create rapport during interactions with human users. In this paper, we introduce our approach to creating rapport following Tickle-Degnen and Rosenberg’s threefactor (positivity, mutual attention and coordination) theory of rapport. By comparing with a previously published virtual agent, the Rapport Agent, we show that our virtual human predicts the timing of backchannel feedback and end-of-turn more precisely, performs more natural behaviors and, thereby creates much stronger feelings of rapport between users and virtual agents.},
annote = {VER DUG-1 system [9] para gerar utterances e replanear

Ver 19
Ver SAIBA pipeline [15]

Used facelab

using BDI to plan what content will be communicated

model for attentive spekar agent to atend and adapt to differnt kind of feedback (RAPPORT)

modelo para determinal o melhor timing para feedback (rule based to complex machine learning aproaches [25,17]

Talks about coordinations and accomodations that happens implicitly or explicitly, from instanteous to longer streatchs [13]

deeback loops by being able to adapt to new circumstances and try new actions, measure the effectiveness in reaching goals

Soeakers needs to interpret form and timing of feedback and signals in order to be able to respond to them in a way that facilitates mutual understanding

Talks about how feedback i simportant, because a history is way better told when the listeners give feedback.

Mention that on the lower level: align same words, pronouncing aline and using similar linguistic strucutre [SEEEEE 18 and 6]. On an higher level coordination, concious opinions are transmited between the speakers

attentive spekars {\~{}}= rapport
feedback elicitaton
feedback interpretation

short vocal-vebal {\&}quot;uh huh{\&}quot; or non verbal (head noding)

Adaptation of utterances to what we believe the listener needs or want 

conversations: mutual coordination and stablish shared beliefs [7]

quoting:
rapport is about being an active listener, i.e. agents taht produce feedback signals in response to user actions [12,14,17,4,5]},
author = {Huang, Lixing and Morency, Louis-Philippe and Gratch, Jonathan},
booktitle = {Intelligent Virtual Agents},
doi = {10.1007/978-3-642-23974-8},
file = {:Users/brunohenriques/Documents/ThesisPapers/Huang, Morency, Gratch/Intelligent Virtual Agents/Huang, Morency, Gratch - 2011 - Virtual Rapport 2.0(2).pdf:pdf},
isbn = {978-3-642-23973-1},
issn = {03029743},
keywords = {Coordination,Mutual attention,Positivity,Rapport,Virtual human},
pages = {68--79},
title = {{Virtual Rapport 2.0}},
url = {http://link.springer.com/10.1007/978-3-642-23974-8 http://www.springerlink.com/index/10.1007/978-3-642-23974-8$\backslash$nhttp://link.springer.com/content/pdf/10.1007/11821830.pdf},
volume = {6895},
year = {2011}
}
@inproceedings{DeKok2012,
annote = {Comparison between developed models:

Objective evaluation
Subjective evaluation

NEED FOR AN UNIFIED METRICS

Seems to talk about metrics, referes machine learning},
author = {de Kok, Iwan and Heylen, Dirk},
booktitle = {Interdisciplinary Workshop on Feedback Behaviors in Dialog},
file = {:Users/brunohenriques/Documents/ThesisPapers/de Kok, Heylen/Interdisciplinary Workshop on Feedback Behaviors in Dialog/de Kok, Heylen - 2012 - A Survey on Evaluation Metrics for Backchannel Prediction Models.pdf:pdf},
pages = {15--18},
title = {{A Survey on Evaluation Metrics for Backchannel Prediction Models}},
url = {http://eprints.eemcs.utwente.nl/22780/},
year = {2012}
}
@article{Lemaignan2014,
annote = {not that interesting for now},
author = {Lemaignan, S{\'{e}}verin and Fink, Julia and Dillenbourg, Pierre},
doi = {10.1145/2559636.2559814},
file = {:Users/brunohenriques/Documents/ThesisPapers/Lemaignan, Fink, Dillenbourg/Proceedings of the 2014 ACMIEEE international conference on Human-robot interaction - HRI '14/Lemaignan, Fink, Dillenbourg - 2014 - The Dynamics of Anthropomorphism in Robotics.pdf:pdf},
isbn = {9781450326582},
journal = {Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction - HRI '14},
pages = {226--227},
title = {{The Dynamics of Anthropomorphism in Robotics}},
url = {http://dl.acm.org/citation.cfm?doid=2559636.2559814},
year = {2014}
}
@article{Dias2011,
abstract = {This paper presents a generic and flexible architecture for emotional agents, with what we consider to be the minimum set of functionalities that allows us to implement and compare different appraisal theories in a given scenario. FAtiMA Modular, the architecture proposed is composed of a core algorithm and by a set of components that add particular functionality (either in terms of appraisal or behaviour) to the architecture, which makes the architecture more flexible and easier to extend.},
annote = {(7) ?},
author = {Dias, J and Mascarenhas, Samuel and Paiva, Ana},
doi = {10.1007/978-3-319-12973-0{\_}3},
file = {:Users/brunohenriques/Documents/ThesisPapers/Dias, Mascarenhas, Paiva/International Workshop on Standards for Emotion Modeling/Dias, Mascarenhas, Paiva - 2011 - Fatima Modular Towards an Agent Architecture with a Generic Appraisal Framework.pdf:pdf},
isbn = {978-3-319-12972-3},
issn = {16113349},
journal = {International Workshop on Standards for Emotion Modeling},
pages = {1--8},
title = {{Fatima Modular: Towards an Agent Architecture with a Generic Appraisal Framework}},
url = {http://www.lorentzcenter.nl/lc/web/2011/464/presentations/Dias.pdf},
year = {2011}
}
@article{Belkaid2014,
abstract = {Job interview simulation with a virtual agents aims at improving people's social skills and supporting professional inclusion. In such simulators, the virtual agent must be capable of representing and reasoning about the user's mental state based on social cues that inform the system about his/her affects and social attitude. In this paper, we propose a formal model of Theory of Mind (ToM) for virtual agent in the context of human-agent interaction that focuses on the affective dimension. It relies on a hybrid ToM that combines the two major paradigms of the domain. Our framework is based on modal logic and inference rules about the mental states, emotions and social relations of both actors. Finally, we present preliminary results regarding the impact of such a model on natural interaction in the context of job interviews simulation.},
annote = {Refere o FAtiMA: not
Referes BDI is a good basis to reprensetn and reson about the interluctor mental state

Not very much related with rapport},
archivePrefix = {arXiv},
arxivId = {1402.5043},
author = {Belkaid, Marwen and Cedex, Cergy and Sabouret, Nicolas},
eprint = {1402.5043},
file = {:Users/brunohenriques/Documents/ThesisPapers/Belkaid, Cedex, Sabouret/Unknown/Belkaid, Cedex, Sabouret - 2012 - A Logical Model of Theory of Mind for Virtual Agents in the Context of Job Interview Simulation.pdf:pdf},
keywords = {affective computing,cognitive models,human-agent interaction,logic-based approaches,serious games,theory of mind},
number = {march},
title = {{A Logical Model of Theory of Mind for Virtual Agents in the Context of Job Interview Simulation}},
url = {http://arxiv.org/abs/1402.5043},
year = {2012}
}
@article{DeMelo2009,
abstract = {Moral emotions have been argued to play a central role in the emergence of cooperation in human-human interactions. This work describes an experiment which tests whether this insight carries to virtual human-human interactions. In particular, the paper describes a repeated-measures experiment where subjects play the iterated prisoner’s dilemma with two versions of the virtual human: (a) neutral, which is the control condition; (b) moral, which is identical to the control condition except that the virtual human expresses gratitude, distress, remorse, reproach and anger through the face according to the action history of the game. Our results indicate that subjects cooperate more with the virtual human in the moral condition and that they perceive it to be more human-like. We discuss the relevance these results have for building agents which are successful in cooperating with humans.},
author = {{De Melo}, Celso M. and Zheng, Liang and Gratch, Jonathan},
doi = {10.1007/978-3-642-04380-2{\_}32},
file = {:Users/brunohenriques/Documents/ThesisPapers/De Melo, Zheng, Gratch/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/De Melo, Zheng, Gratch - 2009 - Expression of Moral Emotions in Cooperating Agents.pdf:pdf},
isbn = {3642043798},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Cooperation,Expression of Emotions,Moral Emotions,Prisoner's Dilemma,Virtual Humans},
pages = {301--307},
title = {{Expression of Moral Emotions in Cooperating Agents}},
volume = {5773 LNAI},
year = {2009}
}
@article{Kopp2013,
annote = {Paper a sugerir solu{\c{c}}{\~{o}}es para fluidez: closeness in both generation and input processing {\&}amp;{\&}amp; incrementalidade nos processos

Interessante o conceito de fluidez. As ideias daqui s{\~{a}}o interessantes para desenvolver uma rarquirteura {\'{a}}gil e adequada pra rapport

complex paper regarding a proposed arquitecture with extensions of the current standard FML BML},
author = {Kopp, Stefan and van Welbergen, Herwin and Yaghoubzadeh, Ramin and Buschmeier, Hendrik},
doi = {10.1007/s12193-013-0130-3},
file = {:Users/brunohenriques/Documents/ThesisPapers/Kopp et al/Journal on Multimodal User Interfaces/Kopp et al. - 2014 - An Architecture for Fluid Real-time Conversational Agents Integrating Incremental Output Generation and Input Proce.pdf:pdf},
issn = {17838738},
journal = {Journal on Multimodal User Interfaces},
keywords = {ASAP,BMLA,Embodied conversational agents architecture,Fluid real-time interaction,Generation-interpretation coordination,Incremental processing},
number = {1},
pages = {97--108},
title = {{An Architecture for Fluid Real-time Conversational Agents: Integrating Incremental Output Generation and Input Processing}},
url = {http://link.springer.com/10.1007/s12193-013-0130-3},
volume = {8},
year = {2014}
}
@article{Rosenthal-VonDerPutten2013,
abstract = {We conducted an fMRI study to investigate emotionality in human-robot interaction. Subjects (N=14) were presented videos showing a human, a robot and an unanimated object, being treated in either an affectionate or a violent way. Violent interaction towards both the robot and the human resulted in similar neural activation patterns in classic limbic structures indicating that both the robot and the human elicit similar emotional reactions. However, differences in neural activity suggest that participants show more negative empathetic concern for the human in a negative situation.},
annote = {not really intersting for me},
archivePrefix = {arXiv},
arxivId = {RosenthalvonderP{\"{u}}tten2013},
author = {{Rosenthal-Von Der P{\"{u}}tten}, Astrid M. and Schulte, Frank P. and Eimler, Sabrina C. and Hoffmann, Laura and Sobieraj, Sabrina and Maderwald, Stefan and Kr{\"{a}}mer, Nicole C. and Brand, Matthias},
doi = {10.1109/HRI.2013.6483578},
eprint = {RosenthalvonderP{\"{u}}tten2013},
file = {:Users/brunohenriques/Documents/ThesisPapers/Rosenthal-Von Der P{\"{u}}tten et al/ACMIEEE International Conference on Human-Robot Interaction/Rosenthal-Von Der P{\"{u}}tten et al. - 2013 - Neural Correlates of Empathy Towards Robots.pdf:pdf},
isbn = {9781467330558},
issn = {21672148},
journal = {ACM/IEEE International Conference on Human-Robot Interaction},
keywords = {empathy,experimental study,functional magnetic resonance imaging,human-robot interaction},
pages = {215--216},
pmid = {18692571},
title = {{Neural Correlates of Empathy Towards Robots}},
year = {2013}
}
@article{Lisetti2013,
abstract = {We discuss our approach to developing a novel modality for the computer-delivery of Brief Motivational Interventions (BMIs) for behavior change in the form of a personalized On-Demand VIrtual Counselor (ODVIC), accessed over the internet. ODVIC is a multimodal Embodied Conversational Agent (ECA) that empathically delivers an evidence-based behavior change intervention by adapting, in real-time, its verbal and nonverbal communication messages to those of the user’s during their interaction. We currently focus our work on excessive alcohol consumption as a target behavior, and our approach is adaptable to other target behaviors (e.g., overeating, lack of exercise, narcotic drug use, non-adherence to treatment). We based our current approach on a successful existing patient-centered brief motivational intervention for behavior change---the Drinker’s Check-Up (DCU)---whose computer-delivery with a text-only interface has been found effective in reducing alcohol consumption in problem drinkers. We discuss the results of users’ evaluation of the computer-based DCU intervention delivered with a text-only interface compared to the same intervention delivered with two different ECAs (a neutral one and one with some empathic abilities). Users rate the three systems in terms of acceptance, perceived enjoyment, and intention to use the system, among other dimensions. We conclude with a discussion of how our positive results encourage our long-term goals of on-demand conversations, anytime, anywhere, with virtual agents as personal health and well-being helpers.},
annote = {Didn'0t finished reading.

Interesting architectures


rapport in a specific context. Important paper},
author = {Lisetti, Christine and Amini, Reza and Yasavur, Ugan and Rishe, Naphtali},
doi = {10.1145/2544103},
file = {:Users/brunohenriques/Documents/ThesisPapers/Lisetti et al/ACM Transactions on Management Information Systems/Lisetti et al. - 2013 - I Can Help You Change! An Empathic Virtual Agent Delivers Behavior Change Health Interventions.pdf:pdf},
isbn = {2158-656X},
issn = {2158656X},
journal = {ACM Transactions on Management Information Systems},
number = {4},
pages = {1--28},
title = {{I Can Help You Change! An Empathic Virtual Agent Delivers Behavior Change Health Interventions}},
volume = {4},
year = {2013}
}
@article{Rosenthal-vonderPutten2013,
abstract = {Although robots are starting to enter into our professional and private lives, little is known about the emotional effects which robots elicit. However, insights into this topic are an important prerequisite when discussing, for example, ethical issues regarding the question of what role we (want to) allow robots to play in our lives. In line with the Media Equation, humans may react towards robots as they do towards humans, making it all the more important to carefully investigate the preconditions and consequences of contact with robots. Based on assumptions on the socialness of reactions towards robots and anecdotal evidence of emotional attachments to robots (e.g. Klamer and BenAllouch in Trappl R. (ed.), Proceedings of EMCSR 2010, Vienna, 2010; Klamer and BenAllouch in Proceedings of the 27th International Conference on Human Factors in Computing Systems (CHI-2010), Atlanta, GA. ACM, New York, 2010; Kr{\"{a}}mer et al. in Appl. Artif. Intell. 25(6):474–502, 2011), we conducted a study that provides further insights into the question of whether humans show emotional reactions towards Ugobe’s Pleo, which is shown in different situations. We used a 2×2 design with one between-subjects factor “prior interaction with the robot” (never seen the robot before vs. 10-minute interaction with the robot) and a within-subject factor “type of video” (friendly interaction video vs. torture video). Following a multi-method approach, we assessed participants’ physiological arousal and self-reported emotions as well as their general evaluation of the videos and the robot. In line with our hypotheses, participants showed increased physiological arousal during the reception of the torture video as compared to the normal video. They also reported fewer positive and more negative feelings after the torture video and expressed empathic concern for the robot. It appears that the acquaintance with the robot does not play a role, as “prior interaction with the robot” showed no effect.},
annote = {not really important},
author = {{Rosenthal-von der P{\"{u}}tten}, Astrid M. and Kr{\"{a}}mer, Nicole C. and Hoffmann, Laura and Sobieraj, Sabrina and Eimler, Sabrina C.},
doi = {10.1007/s12369-012-0173-8},
file = {:Users/brunohenriques/Documents/ThesisPapers/Rosenthal-von der P{\"{u}}tten et al/International Journal of Social Robotics/Rosenthal-von der P{\"{u}}tten et al. - 2013 - An Experimental Study on Emotional Reactions Towards a Robot.pdf:pdf},
isbn = {1875-4791; 1875-4805},
issn = {18754791},
journal = {International Journal of Social Robotics},
keywords = {Emotional response,Empathy,Experimental study,Human-robot interaction,Psychophysiological measures},
number = {1},
pages = {17--34},
title = {{An Experimental Study on Emotional Reactions Towards a Robot}},
url = {http://link.springer.com/10.1007/s12369-012-0173-8},
volume = {5},
year = {2013}
}
@misc{VandenAssem2012,
abstract = {W e examine cooperative behavior when large sums of money are at stake, using data from the television game show Golden Balls. At the end of each episode, contestants play a variant on the classic prisoner’s dilemma for large and widely ranging stakes averaging over {\$}20,000. Cooperation is surprisingly high for amounts that would normally be considered consequential but look tiny in their current context, what we call a “big peanuts” phenomenon. Utilizing the prior interaction among contestants, we find evidence that people have reciprocal preferences. Surprisingly, there is little support for conditional cooperation in our sample. That is, players do not seem to be more likely to cooperate if their opponent might be expected to cooperate. Further, we replicate earlier findings that males are less cooperative than females, but this gender effect reverses for older contestants because men become increasingly cooperative as their age increases.},
author = {van den Assem, M. J. and van Dolder, D. and Thaler, R. H.},
booktitle = {Management Science},
doi = {10.1287/mnsc.1110.1413},
file = {:Users/brunohenriques/Documents/ThesisPapers/van den Assem, van Dolder, Thaler/Management Science/van den Assem, van Dolder, Thaler - 2012 - Split or Steal Cooperative Behavior When the Stakes Are Large.pdf:pdf},
isbn = {0025-1909},
issn = {0025-1909},
keywords = {3000 dr,anchoring,behavior,box 1738,context effects,cooperation,cooperative behavior,correspondence,e,erasmus university of rotterdam,game show,natural experiment,o,p,postal address for manuscript,prisoner,reciprocal,reciprocity,rotterdam,s dilemma,social behavior,social preferences,the netherlands},
number = {1},
pages = {2--20},
pmid = {70894768},
title = {{Split or Steal? Cooperative Behavior When the Stakes Are Large}},
volume = {58},
year = {2012}
}
@article{Reidsma2011,
abstract = {This paper presents our progress in developing a Virtual Human capable of being an attentive speaker. Such a Virtual Human should be able to attend to its interaction partner while it is speaking—and modify its communicative behavior on-the-fly based on what it observes in the behavior of its partner. We report new developments concerning a number of aspects, such as scheduling and interrupting multimodal behavior, automatic classification of listener responses, generation of response eliciting behavior, and strategies for generating appropriate reactions to listener responses. On the basis of this progress, a task-based setup for a responsive Virtual Human was implemented to carry out two user studies, the results of which are presented and discussed in this paper.},
annote = {Scheduling and interrupting multimodal behaviour. Automatic classification of listener responses, generation of response eliciting behaviour and strategies for generating appropriate reactions to listenr response.

modify communicatve behaviour on the fly based on what it observers of its parner.

A speaker may also actively elicit re- sponses using, e.g., face expressions or vocal cues. In short, interlocutors continuously and in coordination with one an- other show attentive speaking and active listening behavior [3, 11]. In

They serve many functions, of which the most important is to neutrally signal that the listener hears that the speaker is talking. A Listener Response having this function is often referred to as a back-channel [13]. Other


In Goodwin’s observations, a speaker does not change
the content of what he says based on the responses from the listener, but rather coordinates the timing of his speech influ- enced by the listener’s responses [22]. Listener




segmentacao {\'{e}} importante mas {\'{e}} importante preparar utterances {\`{A}} priori para poder ter bons timings

faz uma sugestao

model to analys},
author = {Reidsma, Dennis and de Kok, Iwan and Neiberg, Daniel and Pammi, Sathish Chandra and van Straalen, Bart and Truong, Khiet and van Welbergen, Herwin},
doi = {10.1007/s12193-011-0060-x},
file = {:Users/brunohenriques/Documents/ThesisPapers/Reidsma et al/Journal on Multimodal User Interfaces/Reidsma et al. - 2011 - Continuous Interaction With a Virtual Human.pdf:pdf},
issn = {17837677},
journal = {Journal on Multimodal User Interfaces},
keywords = {Attentive speaking,Continuous interaction,Listener responses,Virtual humans},
number = {2},
pages = {97--118},
title = {{Continuous Interaction With a Virtual Human}},
url = {http://link.springer.com/10.1007/s12193-011-0060-x},
volume = {4},
year = {2011}
}
@article{Cappella1990,
abstract = {Offers a critical reaction to the conceptual analysis of rapport by L. Tickle-Degnen and R. Rosenthal (see PA, Vol 79:1371) and discusses the conditions under which the association between nonverbal behavior and rapport will be amplified. ((c) 1997 APA/PsycINFO, all rights reserved)},
annote = {{\'{e}} importante definir o que {\'{e}} cooredna{\c{c}}{\~{a}}o? {\'{E}} s{\'{o}} a positiva que conta? Conta s{\'{o}} para o primeiro factor da positividade de rapport?},
author = {Cappella, Joseph N.},
doi = {10.1207/s15327965pli0104{\_}5},
file = {:Users/brunohenriques/Documents/ThesisPapers/Cappella/Psychological Inquiry/Cappella - 1990 - On Defining Conversational Coordination and Rapport.pdf:pdf},
isbn = {1047-840X},
issn = {1047-840X},
journal = {Psychological Inquiry},
number = {4},
pages = {303--305},
title = {{On Defining Conversational Coordination and Rapport}},
volume = {1},
year = {1990}
}
@article{Chidambaram2012,
abstract = {Social robots have to potential to serve as personal, organizational, and public assistants as, for instance, diet coaches, teacher's aides, and emergency respondents. The success of these robots - whether in motivating users to adhere to a diet regimen or in encouraging them to follow evacuation procedures in the case of a fire - will rely largely on their ability to persuade people. Research in a range of areas from political communication to education suggest that the nonverbal behaviors of a human speaker play a key role in the persuasiveness of the speaker's message and the listeners' compliance with it. In this paper, we explore how a robot might effectively use these behaviors, particularly vocal and bodily cues, to persuade users. In an experiment with 32 participants, we evaluate how manipulations in a robot's use of nonverbal cues affected participants' perceptions of the robot's persuasiveness and their compliance with the robot's suggestions across four conditions: (1) no vocal or bodily cues, (2) vocal cues only, (3) bodily cues only, and (4) vocal and bodily cues. The results showed that participants complied with the robot's suggestions significantly more when it used nonverbal cues than they did when it did not use these cues and that bodily cues were more effective in persuading participants than vocal cues were. Our model of persuasive nonverbal cues and experimental results have direct implications for the design of persuasive behaviors for humanlike robots.},
annote = {Cont{\'{e}}m MONTES DE REFERENCIAS PARA ESTUDOS

Body clues are more effectiv than verbal clues


bodily cues such as proximity, gaze,
gestures, posture, facial expressions, touching and vocal cues
such as vocal tone and expressions [43].},
author = {Chidambaram, Vijay and Chiang, Yueh-Hsuan and Mutlu, Bilge},
doi = {10.1145/2157689.2157798},
file = {:Users/brunohenriques/Documents/ThesisPapers/Chidambaram, Chiang, Mutlu/7th Annual ACMIEEE International Conference on Human-Robot Interaction (HRI '12)/Chidambaram, Chiang, Mutlu - 2012 - Designing Persuasive Robots How Robots Might Persuade People Using Vocal and Nonverbal Cues.pdf:pdf},
isbn = {9781450310635},
issn = {2167-2121},
journal = {7th Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI '12)},
keywords = {compliance,gaze,gestures,nonverbal cues,nonverbal immediacy,persuasion,proximity,vocal tone},
number = {September},
pages = {293--300},
title = {{Designing Persuasive Robots: How Robots Might Persuade People Using Vocal and Nonverbal Cues}},
url = {http://dl.acm.org/citation.cfm?doid=2157689.2157798},
year = {2012}
}
@article{Gratch2007,
abstract = {Recent research has established the potential for virtual characters to establish rapport with humans through simple contingent nonverbal behaviors. We hypothesized that the contingency, not just the frequency of positive feedback is crucial when it comes to creating rapport. The primary goal in this study was evaluative: can an agent generate behavior that engenders feelings of rapport in human speakers and how does this compare to human generated feedback? A secondary goal was to answer the question: Is contingency (as opposed to fre- quency) of agent feedback crucial when it comes to creating feelings of rapport? Results suggest that contingency matters when it comes to creating rapport and that agent generated behavior was as good as human listeners in creating rapport. A “virtual human listener” condition performed worse than other conditions.},
author = {Gratch, Jonathan and Wang, Ning and Gerten, Jillian and Fast, Edward and Duffy, Robin},
doi = {http://dx.doi.org/10.1007/978-3-540-74997-4{\_}12},
file = {:Users/brunohenriques/Documents/ThesisPapers/Gratch et al/Intelligent Virtual Agents/Gratch et al. - 2007 - Creating Rapport With Virtual agents.pdf:pdf},
isbn = {978-3-540-74996-7},
issn = {03029743},
journal = {Intelligent Virtual Agents},
keywords = {evaluation,rapport,virtual agents},
pages = {125--138},
title = {{Creating Rapport With Virtual agents}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-74997-4{\_}12},
year = {2007}
}
@article{Kiesler1996,
abstract = {The authors investigated basic properties of social exchange and interaction with technology in an experiment on cooperation with a human-like computer partner or a real human partner. Talking with a computer partner may trigger social identity feelings or commitment norms. Participants played a prisoner's dilemma game with a confederate or a computer partner. Discussion, inducements to make promises, and partner cooperation varied across trials. On Trial 1, after discussion, most participants proposed cooperation. They kept their promises as much with a text-only computer as with a person, but less with a more human-like computer. Cooperation dropped sharply when any partner avoided discussion. The strong impact of discussion fits a social contract explanation of cooperation following discussion. Participants broke their promises to a computer more than to a person, however, indicating that people make heterogeneous commitments.},
author = {Kiesler, S and Sproull, L and Waters, K},
doi = {10.1037/0022-3514.70.1.47},
file = {:Users/brunohenriques/Documents/ThesisPapers/Kiesler, Sproull, Waters/Journal of personality and social psychology/Kiesler, Sproull, Waters - 1996 - A Prisoner's Dilemma Experiment on Cooperation with People and Human-like Computers.pdf:pdf},
isbn = {Print 0022-3514},
issn = {0022-3514},
journal = {Journal of personality and social psychology},
number = {1},
pages = {47--65},
pmid = {8558408},
title = {{A Prisoner's Dilemma Experiment on Cooperation with People and Human-like Computers.}},
volume = {70},
year = {1996}
}
@article{DeMelo2014,
abstract = {How do people make inferences about other people's minds from their emotion displays? The ability to infer others' beliefs, desires, and intentions from their facial expressions should be especially important in interdependent decision making when people make decisions from beliefs about the others' intention to cooperate. Five experiments tested the general proposition that people follow principles of appraisal when making inferences from emotion displays, in context. Experiment 1 revealed that the same emotion display produced opposite effects depending on context: When the other was competitive, a smile on the other's face evoked a more negative response than when the other was cooperative. Experiment 2 revealed that the essential information from emotion displays was derived from appraisals (e.g., Is the current state of affairs conducive to my goals? Who is to blame for it?); facial displays of emotion had the same impact on people's decision making as textual expressions of the corresponding appraisals. Experiments 3, 4, and 5 used multiple mediation analyses and a causal-chain design: Results supported the proposition that beliefs about others' appraisals mediate the effects of emotion displays on expectations about others' intentions. We suggest a model based on appraisal theories of emotion that posits an inferential mechanism whereby people retrieve, from emotion expressions, information about others' appraisals, which then lead to inferences about others' mental states. This work has implications for the design of algorithms that drive agent behavior in human-agent strategic interaction, an emerging domain at the interface of computer science and social psychology.},
annote = {Referes that it is important to analyse the current context to the the most important rapport in order to provoke the desiderd mind stat in the target's mind.

Refers many experimentes regarding the context in which emotions are displayed and how they are decisive to influence other's action

Refers anoteher experience regaridng reverse appraisal},
author = {de Melo, Celso M. and Carnevale, Peter J. and Read, Stephen J. and Gratch, Jonathan},
doi = {10.1037/a0034251},
file = {:Users/brunohenriques/Documents/ThesisPapers/de Melo et al/Journal of personality and social psychology/de Melo et al. - 2014 - Reading People's Minds from Emotion Expressions in Interdependent Decision Making.pdf:pdf},
isbn = {0022-3514},
issn = {1939-1315},
journal = {Journal of personality and social psychology},
keywords = {10,1037,a0034251,appraisal theories,decision making,doi,dx,emotion expressions,http,org,reverse appraisal,supp,supplemental materials,theory of mind},
number = {1},
pages = {73--88},
pmid = {24079297},
title = {{Reading People's Minds from Emotion Expressions in Interdependent Decision Making}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0034251 http://www.ncbi.nlm.nih.gov/pubmed/24079297},
volume = {106},
year = {2014}
}
@article{Melo2011a,
abstract = {The paper presents a computational model for decision-making in a social dilemma that takes into account the other party{\^{a}}€™s emotion displays. The model is based on data collected in a series of recent studies where participants play the iterated prisoner{\^{a}}€™s dilemma with agents that, even though following the same action strategy, show different emotion displays according to how the game unfolds. We collapse data from all these studies and fit, using maximum likelihood estimation, probabilistic models that predict likelihood of cooperation in the next round given different features. Model 1 predicts based on round outcome alone. Model 2 predicts based on outcome and emotion displays. Model 3 also predicts based on outcome and emotion but, considers contrast effects found in the empirical studies regarding the order with which participants play cooperators and non-cooperators. To evaluate the models, we replicate the original studies but, substitute the humans for the models. The results reveal that Model 3 best replicates human behavior in the original studies and Model 1 does the worst. The results, first, emphasize recent research about the importance of nonverbal cues in social dilemmas and, second, reinforce that people attend to contrast effects in their decision-making. Theoretically, the model provides further insight into how people behave in social dilemmas. Pragmatically, the model could be used to drive an agent that is engaged in a social dilemma with a human (or another agent).},
author = {Melo, Celso M De and Carnevale, Peter and Antos, Dimitrios and Gratch, Jonathan},
file = {:Users/brunohenriques/Documents/ThesisPapers/Melo et al/Fourth Bi-Annual International Conference of the HUMAINE Association on Affective Computing and Intelligent Interaction/Melo et al. - 2011 - A Computer Model of the Interpersonal Effect of Emotion Displayed in a Social Dilemma.pdf:pdf},
isbn = {978-3-642-24599-2},
journal = {Fourth Bi-Annual International Conference of the HUMAINE Association on Affective Computing and Intelligent Interaction},
keywords = {cooperation,emotion,probabilistic model,social dilemma},
title = {{A Computer Model of the Interpersonal Effect of Emotion Displayed in a Social Dilemma}},
year = {2011}
}
@article{Welbergen2012,
abstract = {AsapRealizer/Elckerlyc},
annote = {contem exemplos adicionais de interacao sincronizada
Faz cr{\'{\i}}ticas ao BML},
author = {{Van Welbergen}, Herwin and Reidsma, Dennis and Kopp, Stefan},
doi = {10.1007/978-3-642-33197-8-18},
file = {:Users/brunohenriques/Documents/ThesisPapers/Van Welbergen, Reidsma, Kopp/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Van Welbergen, Reidsma, Kopp - 2012 - An Incremental Multimodal Realizer for Behavior Co-Articulation and Coordination.pdf:pdf},
isbn = {9783642331961},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {175--188},
title = {{An Incremental Multimodal Realizer for Behavior Co-Articulation and Coordination}},
volume = {7502 LNAI},
year = {2012}
}
@article{Leite2012,
abstract = {The idea of autonomous social robots capable of assisting us in our daily lives is becoming more real every day. How- ever, there are still many open issues regarding the social capabilities that those robots should have in order to make daily interactions with humans more natural. For example, the role of affective interactions is still unclear. This paper presents an ethnographic study conducted in an elementary school where 40 children interacted with a social robot ca- pable of recognising and responding empathically to some of the children’s affective states. The findings suggest that the robot’s empathic behaviour affected positively how chil- dren perceived the robot. However, the empathic behaviours should be selected carefully, under the risk of having the op- posite effect. The target application scenario and the par- ticular preferences of children seem to influence the “degree of empathy” that social robots should be endowed with. Categories},
annote = {estudo},
author = {Leite, Iolanda and Castellano, Ginevra and Pereira, Andr{\'{e}} and Martinho, Carlos and Paiva, Ana},
doi = {10.1145/2157689.2157811},
file = {:Users/brunohenriques/Documents/ThesisPapers/Leite et al/HRI '12 Proceedings of the seventh annual ACMIEEE international conference on Human-Robot Interaction/Leite et al. - 2012 - Modelling Empathic Behaviour in a Robotic Game Companion for Children an Ethnographic Study in Real-World Settings.pdf:pdf},
isbn = {9781450310635},
issn = {2167-2121},
journal = {HRI '12 Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction},
keywords = {affect recognition,children,empathy,social robots},
pages = {367--374},
title = {{Modelling Empathic Behaviour in a Robotic Game Companion for Children: an Ethnographic Study in Real-World Settings}},
url = {http://dl.acm.org/citation.cfm?id=2157811},
year = {2012}
}
@article{VanDoorn2015,
abstract = {People often express emotion to influence oth- ers, for instance when making a request. Yet, surprisingly little is known about how such emotional expressions shape compliance. We investigated the interpersonal effects of anger and disappointment on compliance with requests. In Experiments 1 and 2, participants were more willing to offer help and donate to charity when a request was accompanied by disappointment rather than anger or no emotion. In Experiment 3, which involved a behavioral paradigm, emotional expressions trumped the effect of an explicit descriptive norm: Expressions of disappointment fostered generosity despite a non-generous norm, and expressions of anger undermined generosity despite a generous norm. Mediation analyses in Experiments 2 and 3 revealed that disappointment was more effective than anger in eliciting compliance because it was perceived as more appropriate for the context. Findings are discussed in relation to theorizing on social influence and the social functions of emotions.},
author = {van Doorn, Evert a. and van Kleef, Gerben a. and van der Pligt, Joop},
doi = {10.1007/s11031-014-9421-6},
file = {:Users/brunohenriques/Documents/ThesisPapers/van Doorn, van Kleef, van der Pligt/Motivation and Emotion/van Doorn, van Kleef, van der Pligt - 2014 - How Emotional Expressions Shape Prosocial Behavior Interpersonal Effects of Anger and Disap.pdf:pdf},
issn = {0146-7239},
journal = {Motivation and Emotion},
keywords = {anger {\'{a}} disappointment,influence {\'{a}} compliance {\'{a}},interpersonal effects of emotions,{\'{a}} social},
number = {1},
pages = {128--141},
title = {{How Emotional Expressions Shape Prosocial Behavior: Interpersonal Effects of Anger and Disappointment on Compliance with Requests}},
url = {http://link.springer.com/10.1007/s11031-014-9421-6},
volume = {39},
year = {2014}
}
@inproceedings{Wang2010,
abstract = {Communication is more effective and persuasive when par- ticipants establish rapport. Tickle-Degnen and Rosenthal [57] argue rapport arises when participants exhibit mutual attentiveness, positivity and coordination. In this paper, we investigate how these factors relate to perceptions of rap- port when users interact via avatars in virtual worlds. In this study, participants told a story to what they believed was the avatar of another participant. In fact, the avatar was a computer program that systematically manipulated levels of attentiveness, positivity and coordination. In contrast to Tickel-Degnen and Rosenthal’s findings, its impact in a wide range of interpersonal domains includ- ing social engagement [52], classroom learning [22], suc- cess in negotiations [20], improving worker compliance [18], psychotherapeutic effectiveness [59], and improved quality of child care [11]. Recent research in virtual envi- ronments has demonstrated the possibility of translating these findings into computer-mediated (CMC) and human- computer interactions (HCI) where embodied communi- cated behaviors can not only be reproduced but altered in novel ways to perhaps amplify their interpersonal conse- quences [26] [5]. high-levels of mutual attentiveness alone can dramatically lower percep- tions of rapport in avatar communication. Indeed, an agent that attempted to maximize mutual attention performed as poorly as an agent that was designed to convey boredom. Adding positivity and coordination to mutual attentiveness, on the other hand, greatly improved rapport. This work un- veils the dependencies between components of rapport and informs the design of agents and avatars in computer medi- ated communication.},
annote = {How the different componentsof rapport adds up

cont{\'{e}}m exemplos fixes de aplicacao para a introducao},
author = {Wang, Ning and Gratch, Jonathan},
booktitle = {ACM Conference on Human Factors in Computing Systems},
doi = {10.1145/1753326.1753513},
file = {:Users/brunohenriques/Documents/ThesisPapers/Wang, Gratch/ACM Conference on Human Factors in Computing Systems/Wang, Gratch - 2010 - Don't Just Stare at Me!.pdf:pdf},
isbn = {9781605589299},
keywords = {Virtual human,back-channel,gaze,head nod,pos- ture mirroring.,rapport},
pages = {1241--1249},
title = {{Don't Just Stare at Me!}},
year = {2010}
}
@article{Gratch2007a,
abstract = {Emotional bonds don't arise from a simple exchange of facial displays, but often emerge through the dynamic give and take of face-to-face interactions. This article explores the phenomenon of rapport, a feeling of connectedness that seems to arise from rapid and contingent positive feedback between partners and is often associated with socio-emotional processes. Rapport has been argued to lead to communicative efficiency, better learning outcomes, improved acceptance of medical advice and successful negotiations. We provide experimental evidence that a simple virtual character that provides positive listening feedback can induce stronger rapport-like effects than face-to-face communication between human partners. Specifically, this interaction can be more engaging to storytellers than speaking to a human audience, as measured by the length and content of their stories. Springer-Verlag Berlin Heidelberg 2007.},
author = {Gratch, Jonathan and Wang, Ning and Okhmatovskaia, Anya and Lamothe, Francois and Morales, Mathieu and {Van Der Werf}, Rick and Morency, Louis-Philippe},
doi = {10.1007/978-3-540-73110-8{\_}30},
file = {:Users/brunohenriques/Documents/ThesisPapers/Gratch et al/12th International Conference on HumanComputer Interaction/Gratch et al. - 2007 - Can Virtual Humans Be More Engaging than Real Ones.pdf:pdf},
isbn = {9783540731085},
issn = {0302-9743},
journal = {12th International Conference on HumanComputer Interaction},
keywords = {artifical intelligence,emotive computing,science},
number = {1990},
pages = {1--10},
title = {{Can Virtual Humans Be More Engaging than Real Ones?}},
url = {http://www.springerlink.com/index/22J810L671287956.pdf},
year = {2007}
}
@article{Morency2008,
abstract = {During face-to-face interactions, listeners use backchannel feedback such as head nods as a signal to the speaker that the commu- nication is working and that they should continue speaking. Predicting these backchannel opportunities is an important milestone for building engaging and natural virtual humans. In this paper we show how se- quential probabilistic models (e.g., HiddenMarkovModel or Conditional Random Fields) can automatically learn from a database of human-to- human interactions to predict listener backchannels using the speaker multimodal output features (e.g., prosody, spoken words and eye gaze). The main challenges addressed in this paper are automatic selection of the relevant features and optimal feature representation for probabilis- tic models. For prediction of visual backchannel cues (i.e., head nods), our prediction model shows a statistically significant improvement over a previously published approach based on hand-crafted rules.},
annote = {fala de uma proposta em data mining},
author = {Morency, Louis Philippe and de Kok, Iwan and Gratch, Jonathan},
doi = {10.1007/s10458-009-9092-y},
file = {:Users/brunohenriques/Documents/ThesisPapers/Morency, de Kok, Gratch/Intelligent Virtual Agents (Lecture Notes in Artificial Intelligence)/Morency, de Kok, Gratch - 2008 - Predicting Listener Backchannels A Probabilistic Multimodal Approach.pdf:pdf},
isbn = {3540854827},
issn = {13872532},
journal = {Intelligent Virtual Agents (Lecture Notes in Artificial Intelligence)},
pages = {176--190},
title = {{Predicting Listener Backchannels: A Probabilistic Multimodal Approach}},
volume = {5208},
year = {2008}
}
@article{DeMelo2009a,
abstract = {Wrinkles, blushing, sweating and tears are physiological manifestations of emotions in humans. Therefore, the simulation of these phenomena is important for the goal of building believable virtual humans which interact naturally and effectively with humans. This paper describes a real-time model for the simulation of wrinkles, blushing, sweating and tears. A study is also conducted to assess the influence of the model on the perception of surprise, sadness, anger, shame, pride and fear. The study follows a repeated-measures design where subjects compare how well is each emotion expressed by virtual humans with or without these phenomena. The results reveal a significant positive effect on the perception of surprise, sadness, anger, shame and fear. The relevance of these results is discussed for the fields of virtual humans and expression of emotions.},
annote = {Real time model of simulation of wrinkes (permanente or temporary), blushing, sweat e tears},
author = {de Melo, Celso and Gratch, Jonathan},
doi = {10.1007/978-3-642-04380-2{\_}23},
file = {:Users/brunohenriques/Documents/ThesisPapers/de Melo, Gratch/Intelligent Virtual Agents (IVA) 2009 Conference/de Melo, Gratch - 2009 - Expression of Emotions using Wrinkles, Blushing, Sweating and Tears.pdf:pdf},
isbn = {978-3-642-04379-6},
journal = {Intelligent Virtual Agents (IVA) 2009 Conference},
keywords = {blushing,expression of emotions,sweating,tears,wrinkles},
pages = {188--200},
title = {{Expression of Emotions using Wrinkles, Blushing, Sweating and Tears}},
url = {http://www.springerlink.com/index/A0U3124756083H67.pdf},
year = {2009}
}
@article{Poppe2013,
abstract = {The Switching Wizard of Oz (SWOZ) is a setup to evaluate human behavior synthesis algorithms in online face-to-face interactions. Conversational partners are represented to each other as virtual agents, whose animated behavior is either based on a synthesis algorithm, or driven by the actual behavior of the conversational partner. Human and algorithm have the same expression capabilities. The source is switched at random intervals, which means that the algorithm's behavior can only be identified when it deviates from what is regarded as appropriate. The SWOZ approach is especially suitable for the controlled evaluation of synthesis algorithms that consider a limited set of behaviors. We evaluate a backchannel synthesis algorithm for speaker-listener dialogs using an asymmetric version of the framework. Human speakers talk to virtual listeners, that are either controlled by human listeners or by an algorithm. Speakers indicate when they feel they are no longer talking to a human listener. Analysis of these responses reveals patterns of inappropriate behavior in terms of quantity and timing of backchannels. These insights can be used to improve synthesis algorithms. {\^{A}}© 2013 OpenInterface Association.},
author = {Poppe, Ronald and ter Maat, Mark and Heylen, Dirk},
doi = {10.1007/s12193-013-0131-2},
file = {:Users/brunohenriques/Documents/ThesisPapers/Poppe, ter Maat, Heylen/Journal on Multimodal User Interfaces/Poppe, ter Maat, Heylen - 2013 - Switching Wizard of Oz for the Online Evaluation of Backchannel Behavior.pdf:pdf},
issn = {1783-7677},
journal = {Journal on Multimodal User Interfaces},
keywords = {backchannels,behavior synthesis,listening behavior,online evaluation,social,wizard of oz},
pages = {109--117},
title = {{Switching Wizard of Oz for the Online Evaluation of Backchannel Behavior}},
url = {http://link.springer.com/10.1007/s12193-013-0131-2},
year = {2013}
}
@incollection{Poppe2011,
abstract = {In a perception experiment, we systematically varied the quantity, type and timing of backchannels. Participants viewed stimuli of a real speaker side-by-side with an animated listener and rated how human-like they perceived the latter’s backchannel behavior. In addition, we obtained measures of appropriateness and optionality for each backchannel from key strokes. This approach allowed us to analyze the influence of each of the factors on entire fragments and on individual backchannels. The originally performed type and timing of a backchannel appeared to be more human-like, compared to a switched type or random timing. In addition, we found that nods are more often appropriate than vocalizations. For quantity, too few or too many backchannels per minute appeared to reduce the quality of the behavior. These findings are important for the design of algorithms for the automatic generation of backchannel behavior for artificial listeners.},
annote = {paper regarding timing and that stuff

FIND ANOTHER PAPER REGARDING TIMING AND ALL THIS

telling that the backchannels are mostly optional seems important to tell

O OBJECTIVO {\'{E}} CRIAR ARTICIAL LISTENER

DOIS PAPERS A USAR O YUCK BUTTON},
author = {Poppe, Ronald and Truong, Khiet P. and Heylen, Dirk},
booktitle = {Intelligent Virtual Agents - 10th International Conference, IVA 2011, Reykjavik, Iceland, September 15-17, 2011. Proceedings},
doi = {10.1007/978-3-642-23974-8{\_}25},
file = {:Users/brunohenriques/Documents/ThesisPapers/Poppe, Truong, Heylen/Intelligent Virtual Agents - 10th International Conference, IVA 2011, Reykjavik, Iceland, September 15-17, 2011. Proceedings/Poppe, Truong, Heylen - 2011 - Backchannels Quantity, Type and Timing Matters.pdf:pdf},
isbn = {978-3-642-23973-1},
pages = {228--239},
title = {{Backchannels: Quantity, Type and Timing Matters}},
url = {http://dx.doi.org/10.1007/978-3-642-23974-8{\_}25},
volume = {6895},
year = {2011}
}
@article{Truong2011,
abstract = {Backchannels (BCs) are short vocal and visual listener responses that$\backslash$nsignal attention, interest, and understanding to the speaker. Previous$\backslash$nstudies have investigated BC prediction in telephone-style dialogs$\backslash$nfrom prosodic cues. In contrast, we consider spontaneous face-to-face$\backslash$ndialogs. The additional visual modality allows speaker and listener$\backslash$nto monitor each other's attention continuously, and we hypothesize$\backslash$nthat this affects the BC-inviting cues. In this study, we investigate$\backslash$nhow gaze, in addition to prosody, can cue BCs. Moreover, we focus$\backslash$non the type of BC performed, with the aim to find out whether vocal$\backslash$nand visual BCs are invited by similar cues. In contrast to telephone-style$\backslash$ndialogs, we do not find rising/falling pitch to be a BC-inviting$\backslash$ncue. However, in a face-to-face setting, gaze appears to cue BCs.$\backslash$nIn addition, we find that mutual gaze occurs significantly more often$\backslash$nduring visual BCs. Moreover, vocal BCs are more likely to be timed$\backslash$nduring pauses in the speaker's speech.},
author = {Truong, Khiet P. and Poppe, Ronald and {De Kok}, Iwan and Heylen, Dirk},
file = {:Users/brunohenriques/Documents/ThesisPapers/Truong et al/Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH/Truong et al. - 2011 - A Multimodal Analysis of Vocal and Visual Backchannels in Spontaneous Dialogs.pdf:pdf},
isbn = {19909772 (ISSN)},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Backchannel,Continuer,Gaze,Head nod,Listener response,Pitch,Prediction,Vocalization},
pages = {2973--2976},
title = {{A Multimodal Analysis of Vocal and Visual Backchannels in Spontaneous Dialogs}},
year = {2011}
}
@inproceedings{Melo2011,
abstract = {There is now considerable evidence in social psychology, economics, and related disciplines that emotion plays an important role in negotiation. For example, humans make greater concessions in negotiation to an opposing human who expresses anger, and they make fewer concessions to an opponent who expresses happiness, compared to a no-emotion-expression control. However, in AI, despite the wide interest in negotiation as a means to resolve differences between agents and humans, emotion has been largely ignored. This paper explores whether expression of anger or happiness by computer agents, in a multi- issue negotiation task, can produce effects that resemble effects seen in human-human negotiation. The paper presents an experiment where participants play with agents that express emotions (anger vs. happiness vs. control) through different modalities (text vs. facial displays). An important distinction in our experiment is that participants are aware that they negotiate with computer agents. The data indicate that the emotion effects observed in past work with humans also occur in agent-human negotiation, and occur independently of modality of expression. The implications of these results are discussed for the fields of automated negotiation, intelligent virtual agents and artificial intelligence.},
author = {Melo, Celso M De and Carnevale, Peter and Gratch, Jonathan},
booktitle = {Proceeding AAMAS '11 The 10th International Conference on Autonomous Agents and Multiagent Systems},
file = {:Users/brunohenriques/Documents/ThesisPapers/Melo, Carnevale, Gratch/Proceeding AAMAS '11 The 10th International Conference on Autonomous Agents and Multiagent Systems/Melo, Carnevale, Gratch - 2011 - The Effect of Expression of Anger and Happiness in Computer Agents on Negotiations with Humans.pdf:pdf},
isbn = {0-9826571-7-X 978-0-9826571-7-1},
number = {Aamas},
pages = {937--944},
title = {{The Effect of Expression of Anger and Happiness in Computer Agents on Negotiations with Humans}},
url = {http://www.aamas-conference.org/Proceedings/aamas2011/papers/A7{\_}R78.pdf},
year = {2011}
}
@article{Riek2009,
abstract = {People use imitation to encourage each other during conversation. We have conducted an experiment to in- vestigate how imitation by a robot affect people’s perceptions of their conversation with it. The robot operated in one of threeways: full head gesture mimicking, partial head gesture mimicking (nodding), and non-mimicking (blinking). Participants rated how satisfied they were with the interaction.We hypothesized that participants in the full head gesture condition will rate their interaction the most positively, followed by the partial and non-mimicking conditions. We also performed gesture analysis to see if any differences existed between groups, and did find that men made significantly more gestures than women while interacting with the robot. Finally, we interviewed participants to try to ascertain additional insight into their feelings of rapport with the robot, which revealed a number of valuable insights.},
annote = {Study of rapport in real time

people use imitation to encourage each other dur- ing conversation. We have conducted an experiment to in- vestigate how imitation by a robot affect people’s percep- tions of their conversation with it

full head gesture mimicking, partial head ges- ture mimicking (nodding), and non-mimicking (blinking).

and did find that men made signifi- cantly more gestures than women while interacting with the robot},
author = {Riek, Laurel D. and Paul, Philip C. and Robinson, Peter},
doi = {10.1007/s12193-009-0028-2},
file = {:Users/brunohenriques/Documents/ThesisPapers/Riek, Paul, Robinson/Journal on Multimodal User Interfaces/Riek, Paul, Robinson - 2009 - When My Robot Smiles at me Enabling Human-Robot Rapport via Real-Time Head Gesture Mimicry.pdf:pdf},
issn = {1783-7677},
journal = {Journal on Multimodal User Interfaces},
keywords = {19,affective computing,emotionally conveying,empathy,expressions,facial,forms of expressive empathy,human-robot interaction,is known as,of the most basic,one,social robotics,understand what others are},
number = {1-2},
pages = {99--108},
title = {{When My Robot Smiles at me: Enabling Human-Robot Rapport via Real-Time Head Gesture Mimicry}},
url = {http://link.springer.com/10.1007/s12193-009-0028-2},
volume = {3},
year = {2009}
}
@article{Kok2012,
annote = {machine learning!!!!!!! SO MANY STUFF

ver IterativePErceptual Learning

ver referencia [5] e [9]

[5] -{\&}gt; 
“A probabilistic multimodal
approach for predicting listener backchannels,”},
author = {de Kok, Iwan and Poppe, Ronald and Heylen, Dirk},
file = {:Users/brunohenriques/Documents/ThesisPapers/Kok, Poppe, Heylen/Journal on Multimodal User Interfaces/Kok, Poppe, Heylen - 2012 - Iterative Perceptual Learning for Social Behavior Synthesis.pdf:pdf},
journal = {Journal on Multimodal User Interfaces},
keywords = {backchannel,machine learning,perceptual evaluation,social behavior synthesis},
number = {3},
pages = {1--9},
title = {{Iterative Perceptual Learning for Social Behavior Synthesis}},
url = {http://doc.utwente.nl/79712/},
volume = {8},
year = {2012}
}
@article{LernerJenniferS.YeLiPiercarloValdesolo2015,
abstract = {Anyone whose interests lie in real-life decision processes is bound to note the oft times disturbing role that emotions play in such processes, particularly in the areas of assessment of information and long-range planning. Instead of simply dismissing emotions as noisome, irrational agents in the decision making process, one needs to obtain an understanding of their nature and how they influence the decision making process in order to acquire better control of them. This paper proposes a model of emotions based primarily on the following assumptions: (1) The whole set of emotions forms a system that is evolutionally developed and generically programmed - a system that serves the purpose of making decisions that are appropriate to the kinds of environments that can be characterized as primitive and wild. (2) The non-emotional, more analytical decision system is a product of a much later period in evolution which, along with other higher cognitive-analytical functions, developed primarily to supplement, but not to replace, the emotion system by covering its shortcomings. Thus, even though these two systems are often in conflict, the cognitive decision system does not operate without the help of the emotion system; without desires, loves, and hates there hardly would be utilities. (3) The first assumption gives rise to the possibility of studying the emotion system as a purposeful, rational decision system in its own right.},
annote = {not interesting for now},
author = {Lerner, JS and Li, Y and Valdesolo, P and Kassam, KS},
doi = {10.1016/0001-6918(80)90026-8},
file = {:Users/brunohenriques/Documents/ThesisPapers/Lerner et al/Annual Review of Psychology/Lerner et al. - 2015 - Emotion and Decision Making.pdf:pdf},
isbn = {0001-6918},
issn = {00016918},
journal = {Annual Review of Psychology},
keywords = {affect,appraisal tendency,behavioral economics,choice,judgment,mood},
number = {1-3},
pages = {799--823},
pmid = {25251484},
title = {{Emotion and Decision Making}},
url = {http://www.annualreviews.org/eprint/vVKIPZU5r9dTCzbgdD3M/full/10.1146/annurev-psych-010213-115043},
volume = {66},
year = {2015}
}
@article{Mutlu2006,
abstract = {Engaging storytelling is a necessary skill for humanoid robots if they are to be used in education and entertainment applications. Storytelling requires that the humanoid robot be aware of its audience and able to direct its gaze in a natural way. In this paper, we explore how human gaze can be modeled and implemented on a humanoid robot to create a natural, human-like behavior for storytelling. Our gaze model integrates data collected from a human storyteller and a discourse structure model developed by Cassell and her colleagues for human-like conversational agents (1994). We used this model to direct the gaze of a humanoid robot, Honda's ASIMO, as he recited a Japanese fairy tale using a pre-recorded human voice. We assessed the efficacy of this gaze algorithm by manipulating the frequency of ASIMO's gaze between two participants and used pre and post questionnaires to assess whether participants evaluated the robot more positively and did better on a recall task when ASIMO looked at them more. We found that participants performed significantly better in recalling ASIMO's story when the robot looked at them more. Our results also showed significant differences in how men and women evaluated ASIMO based on the frequency of gaze they received from the robot. Our study adds to the growing evidence that there are many commonalities between human-human communication and human-robot communication},
annote = {Context: storytelling

role of gaze in storytelling and how to create human-like behaviour for storytelling.


We found that participants performed significantly better in recalling ASIMO’s story when the robot looked at them more.

Our results also showed significant differ- ences in how men and women evaluated ASIMO based on the frequency of gaze they received from the robot. 

Our study adds to the growing evidence that there are many commonalities between human-human communication and human-robot communication.

Our research on humanoid robots proposes a framework of five design variables—gaze, gesture, proximity to a human partner, subtle movements of the body, speech and sound—that feature strongly in the interaction design of compelling human-robot education and entertainment applications.


Studies of gaze suggest that people who look more frequently
at others are more likely to be judged favorably [3].


In our experiment, female subjects who were
gazed at more had better recall of the story although this effect
was not present for men.

To our
surprise, female subjects felt more positively about the robot
when they were gazed at less.

Makes lots of remarks on impact of gaze on critical social functions. Makes some remarks on gender difference on behaviour and reception of gaze behaviours.


Gaze affects perfomance on the recall task which is relevant on applications where information is to be conveyed (e.g., education)

Entertainment, in particular, can often be scripted, reducing the need for sensing and robustness to changes in the environment.},
author = {Mutlu, Bilge and Forlizzi, Jodi and Hodgins, Jessica},
doi = {10.1109/ICHR.2006.321322},
file = {:Users/brunohenriques/Documents/ThesisPapers/Mutlu, Forlizzi, Hodgins/Proceedings of the 2006 6th IEEE-RAS International Conference on Humanoid Robots, HUMANOIDS/Mutlu, Forlizzi, Hodgins - 2006 - A Storytelling Robot Modeling and Evaluation of Human-like Gaze Behavior.pdf:pdf},
isbn = {142440200X},
journal = {Proceedings of the 2006 6th IEEE-RAS International Conference on Humanoid Robots, HUMANOIDS},
pages = {518--523},
title = {{A Storytelling Robot: Modeling and Evaluation of Human-like Gaze Behavior}},
year = {2006}
}
@article{Be2003,
author = {Adolphs, Ralph},
file = {:Users/brunohenriques/Documents/ThesisPapers/Adolphs/Nature Review Neuroscience/Adolphs - 2003 - Cognitive Neuroscience of Human Social Behaviour.pdf:pdf},
journal = {Nature Review Neuroscience},
number = {3},
pages = {165--178},
title = {{Cognitive Neuroscience of Human Social Behaviour}},
volume = {4},
year = {2003}
}
@article{Anderson2013,
abstract = {The TARDIS project aims to build a scenario-based serious-game simulation platform for NEETs and job-inclusion associations that supports social training and coaching in the context of job interviews. This paper presents the general architecture of the TARDIS job interview simulator, and the serious game paradigm that we are developing.},
author = {Anderson, Keith and Andr{\'{e}}, Elisabeth and Baur, T. and Bernardini, Sara and Chollet, M. and Chryssafidou, E. and Damian, I. and Ennis, C. and Egges, A. and Gebhard, P. and Jones, H. and Ochs, M. and Pelachaud, C. and Porayska-Pomsta, Ka{\'{s}}ka and Rizzo, P. and Sabouret, Nicolas},
doi = {10.1007/978-3-319-03161-3{\_}35},
file = {:Users/brunohenriques/Documents/ThesisPapers/Anderson et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Anderson et al. - 2013 - The TARDIS Framework Intelligent Virtual Agents for Social Coaching in Job Interviews.pdf:pdf},
isbn = {9783319031606},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {476--491},
title = {{The TARDIS Framework: Intelligent Virtual Agents for Social Coaching in Job Interviews}},
volume = {8253 LNCS},
year = {2013}
}
@article{Skantze2013,
abstract = {This paper investigates forms and functions of user feedback in a map task dialogue between a human and a robot, where the robot is the instruction-giver and the human is the instruction- follower. First, we investigate how user acknowledgements in task-oriented dialogue signal whether an activity is about to be initiated or has been completed. The parameters analysed include the users' lexical and prosodic realisation as well as gaze direction and response timing. Second, we investigate the relation between these parameters and the perception of uncertainty. Copyright © 2013 ISCA.},
author = {Skantze, Gabriel and Oertel, Catharine and Hjalmarsson, Anna},
file = {:Users/brunohenriques/Documents/ThesisPapers/Skantze, Oertel, Hjalmarsson/Feedback/Skantze, Oertel, Hjalmarsson - 2013 - User Feedback in Human-Robot Interaction Prosody, Gaze and Timing.pdf:pdf},
journal = {Feedback},
pages = {5},
title = {{User Feedback in Human-Robot Interaction: Prosody, Gaze and Timing}},
volume = {4},
year = {2013}
}
@article{Tsai2012,
abstract = {In social psychology, emotional contagion describes the widely observed phenomenon of one person’s emotions mimicking surrounding people’s emotions [10]. In this paper, we perform a battery of experiments to explore the existence of agent-human emotional contagion. The first study is a between-subjects design, wherein subjects were shown an image of a character’s face with either a neutral or happy expression. Findings indicate that even a still image induces a very strong increase in self-reported happiness between Neutral and Happy conditions with all characters tested. In a second study, we examine the effect of a virtual character’s presence in a strategic decision by presenting subjects with a modernized Stag Hunt game. Our experiments show that the contagion effect is substantially dampened and does not cause a consistent impact on behavior. A third study explores the impact of the strategic situation within the Stag Hunt and conducts the same experiment using a description of the same strategic situation with the decision already made. We find that the emotional impact returns, implying that the contagion effect is substantially lessened in the presence of a strategic decision.},
author = {Tsai, Jason and Bowring, Emma and Marsella, Stacy and Wood, Wendy and Tambe, Milind},
doi = {10.1007/978-3-642-33197-8{\_}8},
file = {:Users/brunohenriques/Documents/ThesisPapers/Tsai et al/Intelligent Virtual Agents - 12th International Conference, IVA 2012, Santa Cruz, CA, USA, September, 12-14, 2012. Proceedings/Tsai et al. - 2012 - A Study of Emotional Contagion with Virtual Characters.pdf:pdf},
isbn = {978-3-642-33196-1},
journal = {Intelligent Virtual Agents - 12th International Conference, IVA 2012, Santa Cruz, CA, USA, September, 12-14, 2012. Proceedings},
pages = {81--88},
title = {{A Study of Emotional Contagion with Virtual Characters}},
url = {http://dx.doi.org/10.1007/978-3-642-33197-8{\_}8},
volume = {7502},
year = {2012}
}
@inproceedings{Wang2009,
abstract = {How to build virtual agents that establish rapport with human? According to Tickle-Degnen and Rosenthal, the three essential components of rapport are mutual attentiveness, positivity and coordination. In our previous work, we designed an embodied virtual agent to establish rapport with a human speaker by providing rapid and contingent nonverbal feedback. How do we know that a human speaker is feeling a sense of rapport? In this paper, we focus on the positivity component of rapport by investigating the relationship of human speakers' facial expressions on the establishment of rapport. We used an automatic facial expression coding tool called CERT to analyze the human dyad interactions and human-virtual human interactions. Results show that recognizing positive facial displays alone may be insufficient and that recognized negative facial displays was more diagnostic in assessing the level of rapport between participants.},
annote = {analisaram facial expression usando o CERT para identicicar se o rapport esta a ser feito como deve ser},
author = {Wang, Ning and Gratch, Jonathan},
booktitle = {Interaction and Workshops, 2009. ACII 2009},
doi = {10.1109/ACII.2009.5349514},
file = {:Users/brunohenriques/Documents/ThesisPapers/Wang, Gratch/Interaction and Workshops, 2009. ACII 2009/Wang, Gratch - 2009 - Rapport and Facial Expression.pdf:pdf},
isbn = {978-1-4244-4800-5},
pages = {1--6},
title = {{Rapport and Facial Expression}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5349514$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5349514},
year = {2009}
}
@incollection{Buschmeier2011,
abstract = {Rapport, the feeling of being “in sync” with your conversational partners, is argued to underlie many desirable social effects. By generating proper verbal and nonverbal behaviors, virtual humans have been seen to create rapport during interactions with human users. In this paper, we introduce our approach to creating rapport following Tickle-Degnen and Rosenberg’s threefactor (positivity, mutual attention and coordination) theory of rapport. By comparing with a previously published virtual agent, the Rapport Agent, we show that our virtual human predicts the timing of backchannel feedback and end-of-turn more precisely, performs more natural behaviors and, thereby creates much stronger feelings of rapport between users and virtual agents.},
annote = {In this paper, we introduce our
approach to creating rapport following Tickle-Degnen and Rosenberg’s three-factor (positivity, mutual attention and coordination) theory of rapport. ???},
author = {Huang, Lixing and Morency, Louis-Philippe and Gratch, Jonathan},
booktitle = {Intelligent Virtual Agents},
doi = {10.1007/978-3-642-23974-8},
file = {:Users/brunohenriques/Documents/ThesisPapers/Huang, Morency, Gratch/Intelligent Virtual Agents/Huang, Morency, Gratch - 2011 - Virtual Rapport 2.0.pdf:pdf},
isbn = {978-3-642-23973-1},
issn = {03029743},
keywords = {Coordination,Mutual attention,Positivity,Rapport,Virtual human},
pages = {68--79},
title = {{Virtual Rapport 2.0}},
url = {http://link.springer.com/10.1007/978-3-642-23974-8},
volume = {6895},
year = {2011}
}
@article{Melo2012,
author = {Melo, Celso M De and Gratch, Jonathan and Carnevale, Peter and Read, Stephen},
file = {:Users/brunohenriques/Documents/ThesisPapers/Melo et al/34th Annual Meeting of the Cognitive Science Society (CogSci) 2012/Melo et al. - 2012 - Reverse Appraisal The Importance of Appraisals for the Effect of Emotion Displays on People’s Decision Making in.pdf:pdf},
journal = {34th Annual Meeting of the Cognitive Science Society (CogSci) 2012},
keywords = {appraisal theories,decision making,dilemma,emotion displays,reverse appraisal,social},
pages = {270--275},
title = {{Reverse Appraisal: The Importance of Appraisals for the Effect of Emotion Displays on People’s Decision Making in a Social Dilemma}},
year = {2012}
}
@article{Baxter2014,
abstract = {In this contribution, we describe a method of analysing and interpreting the direction and timing of a human’s gaze over time towards a robot whilst interacting. Based on annotated video recordings of the interactions, this post-hoc analysis can be used to determine how this gaze behaviour changes over the course of an interaction, following from the observa- tion that humans change their behaviour towards the robot on the time-scale of individual interactions. We posit that given these circumstances, this measure may be used as a proxy (among others) for engagement in the interaction or the human’s attribution of social agency to the robot. Ap- plication of this method to a sample of unstructured child- robot interactions demonstrates its use, and justifies its util- isation in future studies.},
annote = {objective characterisation of the gaze behaviour of the human towards the robot during an interaction, and explore how this analysis can shape the assessment of the human’s behaviour, and the state of the interaction itself.

human gaze behaviour over the course of an interaction can provide useful information regarding the state of the interaction, and also the attitude of the human towards the robot.},
author = {Baxter, Paul and Kennedy, James and Vollmer, Anna-Lisa and de Greeff, Joachim and Belpaeme, Tony},
doi = {10.1145/2559636.2559829},
file = {:Users/brunohenriques/Documents/ThesisPapers/Baxter et al/Proceedings of the 2014 ACMIEEE international conference on Human-robot interaction - HRI '14/Baxter et al. - 2014 - Tracking Gaze over Time in HRI as a Proxy for Engagement and Attribution of Social Agency.pdf:pdf},
isbn = {9781450326582},
issn = {21672148},
journal = {Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction - HRI '14},
pages = {126--127},
title = {{Tracking Gaze over Time in HRI as a Proxy for Engagement and Attribution of Social Agency}},
url = {http://dl.acm.org/citation.cfm?doid=2559636.2559829},
year = {2014}
}
@article{Becker-Asano2013,
annote = {estudam o impacto de um agente com emo{\c{c}}{\~{o}}es},
author = {Becker-Asano, Christian and Stahl, Philip and Ragni, Marco and Courgeon, Matthieu and Martin, Jean Claude and Nebel, Bernhard},
doi = {10.1007/978-3-642-40415-3{\_}36},
file = {:Users/brunohenriques/Documents/ThesisPapers/Becker-Asano et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Becker-Asano et al. - 2013 - An Affective Virtual Agent Providing Embodied Feedback in the Paired Associate Task System Design and Evalu.pdf:pdf},
isbn = {9783642404146},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {406--415},
title = {{An Affective Virtual Agent Providing Embodied Feedback in the Paired Associate Task: System Design and Evaluation}},
volume = {8108 LNAI},
year = {2013}
}
@article{Belpaeme2013,
abstract = {Child-Robot Interaction (cHRI) is a promising point of en- try into the rich challenge that social HRI is. Starting from three years of experiences gained in a cHRI research project, this paper offers a view on the opportunities offered by letting robots interact with children rather than with adults and having the interaction in real-world circum- stances rather than lab settings. It identifies the main challenges which face the field of cHRI: the technical challenges, while tremendous, might be overcome by moving away from the classical perspective of seeing so- cial cognition as residing inside an agent, to seeing social cognition as a continuous and self-correcting interaction between two agents.},
author = {Belpaeme, Tony and Baxter, Paul and Greeff, Joachim De and Kennedy, James and Read, Robin and Looije, Rosemarijn and Neerincx, Mark and Baroni, Ilaria and Zelati, Mattia Coti},
file = {:Users/brunohenriques/Documents/ThesisPapers/Belpaeme et al/Social Robotics/Belpaeme et al. - 2013 - Child-Robot Interaction Perspectives and Challenges.pdf:pdf},
journal = {Social Robotics},
keywords = {Education,Robotics},
number = {September 2015},
pages = {452--459},
title = {{Child-Robot Interaction : Perspectives and Challenges}},
volume = {8239},
year = {2013}
}
@article{Antos2011,
abstract = {We present a novel methodology for decision- making by computer agents that leverages a com- putational concept of emotions. It is believed that emotions help living organisms perform well in complex environments. Can we use them to im- prove the decision-making performance of com- puter agents? We explore this possibility by for- mulating emotions as mathematical operators that serve to update the relative priorities of the agent’s goals. The agent uses rudimentary domain knowl- edge to monitor the expectation that its goals are going to be accomplished in the future, and re- acts to changes in this expectation by “experienc- ing emotions.” The end result is a projection of the agent’s long-run utility function, which might be too complex to optimize or even represent, to a time-varying valuation function that is being my- opically maximized by selecting appropriate ac- tions. Our methodology provides a systematic way to incorporate emotion into a decision-theoretic framework, and also provides a principled, domain- independent methodology for generating heuristics in novel situations. We test our agents in simula- tion in two domains: restless bandits and a sim- ple foraging environment. Our results indicate that emotion-based agents outperform other reasonable heuristics for such difficult domains, and closely approach computationally expensive near-optimal solutions, whenever these are computable, yet re- quiring only a fraction of the cost.},
author = {Antos, Dimitrios and Pfeffer, Avi},
doi = {10.5591/978-1-57735-516-8/IJCAI11-016},
file = {:Users/brunohenriques/Documents/ThesisPapers/Antos, Pfeffer/IJCAI International Joint Conference on Artificial Intelligence/Antos, Pfeffer - 2011 - Using Emotions to Enhance Decision-Making.pdf:pdf},
isbn = {9781577355120},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Agent-Based and Multiagent Systems},
pages = {24--30},
title = {{Using Emotions to Enhance Decision-Making}},
year = {2011}
}
@incollection{Gobron2012,
abstract = {Non-verbal communication (NVC) makes up about two-thirds of all communication between two people or between one speaker and a group of listeners. However, this fundamental aspect of communicating is mostly omitted in 3D social forums or virtual world oriented games. This paper proposes an answer by presenting a multi-user 3D-chatting system enriched with NVC relative to motion. This event-based architecture tries to recreate a context by extracting emotional cues from dialogs and derives virtual human potential body expressions from that event triggered context model. We structure the paper by expounding the system architecture enabling the modeling NVC in a multi-user 3D-chatting environment. There, we present the transition from dialog-based emotional cues to body language, and the management of NVC events in the context of a virtual reality client-server system. Finally, we illustrate the results with graphical scenes and a statistical analysis representing the increase of events due to NVC.},
annote = {Looks a good one

. In this paperwe propose the event-based architecture of a working sys- tem –the emotional dynamicmodel being presented in a companion paper.

NVC contains two different elements: human posture and relative positioning, and they have been analyzed to simulate interpersonal relationship between two virtual humans [2].

Parece que eles criaram um modelo que permite intera{\c{c}}{\~{a}}o simultanea entre varios agentes atrav{\'{e}}s de eventos.},
author = {Gobron, St{\'{e}}phane and Ahn, Junghyun and Garcia, David and Silvestre, Quentin and Thalmann, Daniel and Boulic, Ronan},
booktitle = {Articulated Motion and Deformable Objects},
doi = {10.1007/978-3-642-31567-1{\_}6},
file = {:Users/brunohenriques/Documents/ThesisPapers/Gobron et al/Articulated Motion and Deformable Objects/Gobron et al. - 2012 - An Event-Based Architecture to Manage Virtual Human Non-Verbal Communication in 3D Chatting Environment.pdf:pdf},
isbn = {978-3-642-31566-4},
issn = {03029743},
keywords = {3D-chatting,Affective architecture,Avatars,Non-verbal communication,Social agents,Virtual reality},
pages = {58 -- 68},
title = {{An Event-Based Architecture to Manage Virtual Human Non-Verbal Communication in 3D Chatting Environment}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-31567-1{\_}6},
year = {2012}
}
@misc{Putten2009,
abstract = {This study investigates whether humans perceive a higher degree of social presence when interacting with an animated character that displays natural as opposed to no listening behaviors and whether this interacts with people’s believe that they are interacting with an agent or an avatar. In a 2x2 between subjects experimental design 83 participants were either made believe that they encounter an agent, or that they communicate with another participant mediated by an avatar. In fact, in both conditions the communication partner was an autonomous agent that either exhibited high or low behavioral realism. We found that participants experienced equal amounts of presence, regardless of Behavioral interacting realism, however, had with an agent or an avatar. an impact on the subjective feeling of presence: people confronted with a character displaying high behavioral realism reported a higher degree of mutual awareness.},
annote = {contem definicao de embodied conversational agents},
author = {P{\"{u}}tten, Astrid M Von Der and Kr{\"{a}}mer, Nicole C and Gratch, Jonathan},
booktitle = {Design},
file = {:Users/brunohenriques/Documents/ThesisPapers/P{\"{u}}tten, Kr{\"{a}}mer, Gratch/Design/P{\"{u}}tten, Kr{\"{a}}mer, Gratch - 2009 - Who's there Can a Virtual Agent Really Elicit Social Presence.pdf:pdf},
keywords = {avatars,behavioral realism,experimental study,virtual agents},
pages = {1--7},
title = {{Who's there? Can a Virtual Agent Really Elicit Social Presence?}},
year = {2009}
}
@inproceedings{Huang2010,
abstract = {Backchannel feedback is an important kind of nonverbal feedback within face-to-face interaction that signals a person's interest, attention and willingness to keep listening. Learning to predict when to give such feedback is one of the keys to creating natural and realistic virtual humans. Prediction models are traditionally learned from large corpora of annotated face-to-face interactions, but this approach has several limitations. Previously, we proposed a novel data collection method, Parasocial Consensus Sampling, which addresses these limitations. In this paper, we show that data collected in this manner can produce effective learned models. A subjective evaluation shows that the virtual human driven by the resulting probabilistic model significantly outperforms a previously published rule-based agent in terms of rapport, perceived accuracy and naturalness, and it is even better than the virtual human driven by real listeners' behavior in some cases.},
author = {Huang, Lixing and Morency, Louis-philippe and Gratch, Jonathan},
booktitle = {Proceedings of the 10th international Conference on Intelligent Virtual Agents (IVA'10)},
file = {:Users/brunohenriques/Documents/ThesisPapers/Huang, Morency, Gratch/Proceedings of the 10th international Conference on Intelligent Virtual Agents (IVA'10)/Huang, Morency, Gratch - 2010 - Learning Backchannel Prediction Model from Parasocial Consensus Sampling A Subjective Evaluation.pdf:pdf},
isbn = {3-642-15891-9, 978-3-642-15891-9},
keywords = {backchannel prediction,parasocial interaction,virtual human},
pages = {159--172},
title = {{Learning Backchannel Prediction Model from Parasocial Consensus Sampling: A Subjective Evaluation}},
year = {2010}
}
